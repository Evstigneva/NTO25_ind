{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a872a90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  LightGBM –Ω–µ –Ω–∞–π–¥–µ–Ω: module 'matplotlib' has no attribute 'get_data_path'\n",
      "–ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å (RandomForest)...\n",
      "‚úÖ Implicit (ALS) —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω\n",
      "‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ–∑–¥–∞–Ω—ã:\n",
      "   - –î–∞–Ω–Ω—ã–µ: /home/evstigneva/nto25/baseline/data\n",
      "   - –ú–æ–¥–µ–ª–∏: /home/evstigneva/nto25/baseline/output/models\n",
      "   - ALS –º–æ–¥–µ–ª–∏: /home/evstigneva/nto25/baseline/output/models/als\n",
      "   - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: /home/evstigneva/nto25/baseline/output/submissions\n",
      "\n",
      "üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö:\n",
      "   ‚úÖ train.csv - –Ω–∞–π–¥–µ–Ω\n",
      "   ‚úÖ test.csv - –Ω–∞–π–¥–µ–Ω\n",
      "   ‚úÖ users.csv - –Ω–∞–π–¥–µ–Ω\n",
      "   ‚úÖ books.csv - –Ω–∞–π–¥–µ–Ω\n",
      "   ‚úÖ book_genres.csv - –Ω–∞–π–¥–µ–Ω\n",
      "   ‚úÖ genres.csv - –Ω–∞–π–¥–µ–Ω\n",
      "   ‚úÖ book_descriptions.csv - –Ω–∞–π–¥–µ–Ω\n",
      "üéØ –®–ê–ì 1: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\n",
      "============================================================\n",
      "–ü–ê–ô–ü–õ–ê–ô–ù –ü–û–î–ì–û–¢–û–í–ö–ò –î–ê–ù–ù–´–• –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\n",
      "============================================================\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: 268581 -> 156179 —Å—Ç—Ä–æ–∫ (—Ç–æ–ª—å–∫–æ has_read=1)\n",
      "–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\n",
      "–†–∞–∑–º–µ—Ä –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: (159073, 15)\n",
      "\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∏—á–µ–π...\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∏–∑ train.csv...\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 268,581 –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\n",
      "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ has_read: {1: 156179, 0: 112402}\n",
      "\n",
      "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ —Ñ–∏—á...\n",
      "–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ —Ñ–∏—á...\n",
      "–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏—á –∂–∞–Ω—Ä–æ–≤...\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —Ñ–∏—á –∏–∑ —Å–ø–∏—Å–∫–∞ '–Ω–∞ –ø—Ä–æ—á—Ç–µ–Ω–∏–µ'...\n",
      "  - –§–ª–∞–≥ is_in_toread_list...\n",
      "  - –ü—Ä–æ—Å—Ç—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...\n",
      "  –î–æ–±–∞–≤–ª–µ–Ω–æ 5 —Ñ–∏—á –∏–∑ —Å–ø–∏—Å–∫–∞ '–Ω–∞ –ø—Ä–æ—á—Ç–µ–Ω–∏–µ'\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ —É–ø—Ä–æ—â–µ–Ω–Ω—ã—Ö ALS —Ñ–∏—á–µ–π...\n",
      "  –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ ALS —Ñ–∏—á–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã\n",
      "–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∏—á (TF-IDF)...\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä–∞ –∏–∑ /home/evstigneva/nto25/baseline/output/models/tfidf_vectorizer.pkl\n",
      "–î–æ–±–∞–≤–ª–µ–Ω–æ 500 TF-IDF —Ñ–∏—á.\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...\n",
      "–ò–Ω–∂–µ–Ω–µ—Ä–∏—è —Ñ–∏—á –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\n",
      "\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ /home/evstigneva/nto25/baseline/data/processed/processed_features_with_toread.parquet...\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\n",
      "\n",
      "============================================================\n",
      "–ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù–ê!\n",
      "============================================================\n",
      "  - –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏: 156,179\n",
      "  - –¢–µ—Å—Ç–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏: 2,894\n",
      "  - –í—Å–µ–≥–æ —Ñ–∏—á: 524\n",
      "  - –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: /home/evstigneva/nto25/baseline/data/processed/processed_features_with_toread.parquet\n",
      "\n",
      "üéØ –®–ê–ì 2: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\n",
      "============================================================\n",
      "–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\n",
      "============================================================\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ /home/evstigneva/nto25/baseline/data/processed/processed_features_with_toread.parquet...\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: 353.96 MB, 159,073 —Å—Ç—Ä–æ–∫, 524 –∫–æ–ª–æ–Ω–æ–∫\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 159,073 —Å—Ç—Ä–æ–∫ —Å 524 —Ñ–∏—á–∞–º–∏\n",
      "\n",
      "–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º 0.8...\n",
      "–î–∞—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: 2020-09-27 16:17:15\n",
      "–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ: 124,944 —Å—Ç—Ä–æ–∫\n",
      "–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ: 31,235 —Å—Ç—Ä–æ–∫\n",
      "\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ–≤...\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∏–∑ train.csv...\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 268,581 –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\n",
      "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ has_read: {1: 156179, 0: 112402}\n",
      "\n",
      "–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á...\n",
      "–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á...\n",
      "–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á...\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...\n",
      "\n",
      "–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ —Ñ–∏—á–∏: 522\n",
      "–†–∞–∑–º–µ—Ä X_train: (124944, 522)\n",
      "–†–∞–∑–º–µ—Ä X_val: (31235, 522)\n",
      "\n",
      "============================================================\n",
      "–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò\n",
      "============================================================\n",
      "–ò—Å–ø–æ–ª—å–∑—É–µ–º RandomForestRegressor...\n",
      "\n",
      "============================================================\n",
      "–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø\n",
      "============================================================\n",
      "‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π RMSE: 2.0330\n",
      "‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π MAE: 1.3147\n",
      "\n",
      "============================================================\n",
      "–¢–û–ü-20 –í–ê–ñ–ù–ï–ô–®–ò–• –§–ò–ß\n",
      "============================================================\n",
      "  519. user_mean_rating                                   -      0.4\n",
      "  521. book_mean_rating                                   -      0.3\n",
      "  522. book_ratings_count                                 -      0.0\n",
      "   1. user_id                                            -      0.0\n",
      "   4. age                                                -      0.0\n",
      "  13. user_read_toread_ratio                             -      0.0\n",
      "  520. user_ratings_count                                 -      0.0\n",
      "  12. user_toread_count                                  -      0.0\n",
      "  15. book_read_toread_ratio                             -      0.0\n",
      "   5. author_id                                          -      0.0\n",
      "   9. avg_rating                                         -      0.0\n",
      "   2. book_id                                            -      0.0\n",
      "  14. book_toread_count                                  -      0.0\n",
      "   8. publisher                                          -      0.0\n",
      "  265. tfidf_246                                          -      0.0\n",
      "  246. tfidf_227                                          -      0.0\n",
      "   6. publication_year                                   -      0.0\n",
      "  164. tfidf_145                                          -      0.0\n",
      "  329. tfidf_310                                          -      0.0\n",
      "  497. tfidf_478                                          -      0.0\n",
      "\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /home/evstigneva/nto25/baseline/output/models/model_with_toread.pkl\n",
      "‚úÖ –°–ø–∏—Å–æ–∫ —Ñ–∏—á —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ /home/evstigneva/nto25/baseline/output/models/model_features.pkl\n",
      "\n",
      "============================================================\n",
      "–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û\n",
      "============================================================\n",
      "\n",
      "üéØ –®–ê–ì 3: –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\n",
      "============================================================\n",
      "–ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\n",
      "============================================================\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ /home/evstigneva/nto25/baseline/data/processed/processed_features_with_toread.parquet...\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: 353.96 MB, 159,073 —Å—Ç—Ä–æ–∫, 524 –∫–æ–ª–æ–Ω–æ–∫\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 159,073 —Å—Ç—Ä–æ–∫ —Å 524 —Ñ–∏—á–∞–º–∏\n",
      "–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –Ω–∞–±–æ—Ä: 156,179 —Å—Ç—Ä–æ–∫\n",
      "–¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: 2,894 —Å—Ç—Ä–æ–∫\n",
      "\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ–≤...\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∏–∑ train.csv...\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 268,581 –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\n",
      "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ has_read: {1: 156179, 0: 112402}\n",
      "–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á –Ω–∞ –≤—Å–µ—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\n",
      "–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á...\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 522 —Ñ–∏—á –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
      "–§–∏—á–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: (2894, 522)\n",
      "\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ /home/evstigneva/nto25/baseline/output/models/model_with_toread.pkl...\n",
      "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\n",
      "\n",
      "============================================================\n",
      "–ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –°–§–û–†–ú–ò–†–û–í–ê–ù–´\n",
      "============================================================\n",
      "‚úÖ –§–∞–π–ª submission —Å–æ–∑–¥–∞–Ω –≤: /home/evstigneva/nto25/baseline/output/submissions/submission_with_toread_features.csv\n",
      "‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: 2,894\n",
      "‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:\n",
      "   - Min: 0.0000\n",
      "   - Max: 10.0000\n",
      "   - Mean: 7.7594\n",
      "   - Std: 1.7109\n",
      "\n",
      "üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º:\n",
      "   [0.0-2.0): 60 (2.1%)\n",
      "   [2.0-4.0): 49 (1.7%)\n",
      "   [4.0-6.0): 184 (6.4%)\n",
      "   [6.0-8.0): 1,187 (41.0%)\n",
      "   [8.0-10.0): 1,414 (48.9%)\n",
      "\n",
      "üéØ –®–ê–ì 4: –í–ê–õ–ò–î–ê–¶–ò–Ø\n",
      "============================================================\n",
      "–í–ê–õ–ò–î–ê–¶–ò–Ø –§–ê–ô–õ–ê SUBMISSION\n",
      "============================================================\n",
      "–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: 2,894 —Å—Ç—Ä–æ–∫\n",
      "Submission —Ñ–∞–π–ª: 2,894 —Å—Ç—Ä–æ–∫\n",
      "‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –ø—Ä–æ–π–¥–µ–Ω–∞.\n",
      "‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–æ–π–¥–µ–Ω–∞.\n",
      "‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–∞—Ä (user_id, book_id) –ø—Ä–æ–π–¥–µ–Ω–∞.\n",
      "‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π [0, 10] –ø—Ä–æ–π–¥–µ–Ω–∞.\n",
      "‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–∞—Ä –ø—Ä–æ–π–¥–µ–Ω–∞.\n",
      "\n",
      "============================================================\n",
      "–í–ê–õ–ò–î–ê–¶–ò–Ø –£–°–ü–ï–®–ù–ê!\n",
      "============================================================\n",
      "–§–∞–π–ª submission_with_toread_features.csv –∏–º–µ–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏ –≥–æ—Ç–æ–≤ –∫ –∑–∞–≥—Ä—É–∑–∫–µ.\n",
      "–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: 0.09 MB\n",
      "\n",
      "üéØ –®–ê–ì 5: –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´\n",
      "\n",
      "============================================================\n",
      "üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\n",
      "============================================================\n",
      "‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: 159,073 —Å—Ç—Ä–æ–∫, 524 —Ñ–∏—á\n",
      "‚úÖ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: RMSE = 2.0330, MAE = 1.3147\n",
      "‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: 2,894 –ø—Ä–æ–≥–Ω–æ–∑–æ–≤\n",
      "‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è: ‚úÖ –ü–†–û–ô–î–ï–ù–ê\n",
      "üìÅ –§–∞–π–ª submission: /home/evstigneva/nto25/baseline/output/submissions/submission_with_toread_features.csv\n",
      "\n",
      "üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º:\n",
      "   [0.0-2.0): 60 (2.1%)\n",
      "   [2.0-4.0): 49 (1.7%)\n",
      "   [4.0-6.0): 184 (6.4%)\n",
      "   [6.0-8.0): 1,187 (41.0%)\n",
      "   [8.0-10.0): 1,414 (48.9%)\n",
      "\n",
      "============================================================\n",
      "üéâ –ü–ê–ô–ü–õ–ê–ô–ù –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!\n",
      "============================================================\n",
      "üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–î–ì–û–¢–û–í–õ–ï–ù–ù–´–• –î–ê–ù–ù–´–•:\n",
      "–†–∞–∑–º–µ—Ä: (159073, 524)\n",
      "\n",
      "–ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏:\n",
      "  user_id book_id  has_read  rating           timestamp source gender   age  \\\n",
      "0     281  441829       1.0    10.0 2007-04-11 06:09:42  train      1  39.0   \n",
      "1     281  168663       1.0    10.0 2007-04-11 06:10:12  train      1  39.0   \n",
      "2    1851  431081       1.0    10.0 2007-11-26 23:25:24  train      1  36.0   \n",
      "\n",
      "                   title author_id  ... tfidf_490  tfidf_491 tfidf_492  \\\n",
      "0      –§–∞–ª—å—à–∏–≤—ã–µ –∑–µ—Ä–∫–∞–ª–∞    226891  ...  0.151255        0.0       0.0   \n",
      "1     –õ–∞–±–∏—Ä–∏–Ω—Ç –æ—Ç—Ä–∞–∂–µ–Ω–∏–π    226891  ...  0.000000        0.0       0.0   \n",
      "2  –õ—é–±–æ–≤—å –∂–∏–≤–µ—Ç —Ç—Ä–∏ –≥–æ–¥–∞     26123  ...  0.000000        0.0       0.0   \n",
      "\n",
      "  tfidf_493  tfidf_494  tfidf_495 tfidf_496  tfidf_497  tfidf_498  tfidf_499  \n",
      "0  0.000000        0.0        0.0       0.0        0.0        0.0        0.0  \n",
      "1  0.236035        0.0        0.0       0.0        0.0        0.0        0.0  \n",
      "2  0.000000        0.0        0.0       0.0        0.0        0.0        0.0  \n",
      "\n",
      "[3 rows x 524 columns]\n",
      "\n",
      "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ source:\n",
      "source\n",
      "train    156179\n",
      "test       2894\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä –§–ò–ß–ò –ò–ó has_read=0:\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á –∏–∑ has_read=0: 8\n",
      "–ü—Ä–∏–º–µ—Ä—ã:\n",
      "  - is_in_toread_list\n",
      "  - user_toread_count\n",
      "  - user_read_toread_ratio\n",
      "  - book_toread_count\n",
      "  - book_read_toread_ratio\n",
      "  - als_user_score\n",
      "  - als_book_score\n",
      "  - als_dot_product\n"
     ]
    }
   ],
   "source": [
    "# nto25_notebook.ipynb\n",
    "\n",
    "# %% [markdown]\n",
    "# # NTO25 ML Competition Baseline\n",
    "# –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é\n",
    "\n",
    "# %%\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)\n",
    "# !pip install lightgbm pandas numpy scikit-learn tqdm joblib pyarrow\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ LightGBM\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "    print(\"‚úÖ LightGBM —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  LightGBM –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}\")\n",
    "    print(\"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å (RandomForest)...\")\n",
    "    LGB_AVAILABLE = False\n",
    "    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º RandomForest –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    # –°–æ–∑–¥–∞–µ–º –ø—Å–µ–≤–¥–æ–Ω–∏–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\n",
    "    class LGBMRegressorWrapper:\n",
    "        def __init__(self, **kwargs):\n",
    "            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã LightGBM –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã RandomForest\n",
    "            n_estimators = kwargs.get('n_estimators', 100)\n",
    "            max_depth = kwargs.get('num_leaves', 31)\n",
    "            min_samples_split = kwargs.get('min_child_samples', 20)\n",
    "            \n",
    "            self.model = RandomForestRegressor(\n",
    "                n_estimators=min(n_estimators, 100),  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "                max_depth=min(max_depth, 20),\n",
    "                min_samples_split=min_samples_split,\n",
    "                random_state=kwargs.get('seed', 42),\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "        \n",
    "        def fit(self, X, y, eval_set=None, eval_metric=None, callbacks=None, verbose=0):\n",
    "            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º eval_set –¥–ª—è RandomForest\n",
    "            self.model.fit(X, y)\n",
    "        \n",
    "        def predict(self, X):\n",
    "            return self.model.predict(X)\n",
    "        \n",
    "        @property\n",
    "        def feature_importances_(self):\n",
    "            return self.model.feature_importances_\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –ø—Å–µ–≤–¥–æ–Ω–∏–º—ã\n",
    "    lgb = type('lgb', (), {\n",
    "        'LGBMRegressor': LGBMRegressorWrapper,\n",
    "        'early_stopping': lambda **kwargs: None\n",
    "    })()\n",
    "\n",
    "# –î–ª—è ALS (–Ω–µ—è–≤–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å)\n",
    "try:\n",
    "    import implicit\n",
    "    from implicit.als import AlternatingLeastSquares\n",
    "    from scipy.sparse import coo_matrix, csr_matrix\n",
    "    ALS_AVAILABLE = True\n",
    "    print(\"‚úÖ Implicit (ALS) —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ implicit –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. ALS —Ñ–∏—á–∏ –±—É–¥—É—Ç –æ—Ç–∫–ª—é—á–µ–Ω—ã.\")\n",
    "    print(\"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install implicit\")\n",
    "    ALS_AVAILABLE = False\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ implicit: {e}\")\n",
    "    ALS_AVAILABLE = False\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –ö–û–ù–°–¢–ê–ù–¢–´\n",
    "# =============================================================================\n",
    "\n",
    "# --- –§–ê–ô–õ–´ ---\n",
    "TRAIN_FILENAME = \"/home/evstigneva/nto252/train.csv\"\n",
    "TEST_FILENAME = \"/home/evstigneva/nto252/test.csv\" \n",
    "USER_DATA_FILENAME = \"/home/evstigneva/nto252/users.csv\"\n",
    "BOOK_DATA_FILENAME = \"/home/evstigneva/nto252/books.csv\"\n",
    "BOOK_GENRES_FILENAME = \"/home/evstigneva/nto252/book_genres.csv\"\n",
    "GENRES_FILENAME = \"/home/evstigneva/nto252/genres.csv\"\n",
    "BOOK_DESCRIPTIONS_FILENAME = \"/home/evstigneva/nto252/book_descriptions.csv\"\n",
    "SUBMISSION_FILENAME = \"submission_with_toread_features.csv\"\n",
    "TFIDF_VECTORIZER_FILENAME = \"tfidf_vectorizer.pkl\"\n",
    "PROCESSED_DATA_FILENAME = \"processed_features_with_toread.parquet\"\n",
    "ALS_MODEL_FILENAME = \"als_model.pkl\"\n",
    "USER_ITEM_MATRIX_FILENAME = \"user_item_matrix.pkl\"\n",
    "\n",
    "# --- –ù–ê–ó–í–ê–ù–ò–Ø –ö–û–õ–û–ù–û–ö ---\n",
    "COL_USER_ID = \"user_id\"\n",
    "COL_BOOK_ID = \"book_id\"\n",
    "COL_TARGET = \"rating\"\n",
    "COL_SOURCE = \"source\"\n",
    "COL_PREDICTION = \"rating_predict\"\n",
    "COL_HAS_READ = \"has_read\"\n",
    "COL_TIMESTAMP = \"timestamp\"\n",
    "\n",
    "# –§–∏—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–±–∞–∑–æ–≤—ã–µ)\n",
    "F_USER_MEAN_RATING = \"user_mean_rating\"\n",
    "F_USER_RATINGS_COUNT = \"user_ratings_count\"\n",
    "F_BOOK_MEAN_RATING = \"book_mean_rating\"\n",
    "F_BOOK_RATINGS_COUNT = \"book_ratings_count\"\n",
    "F_AUTHOR_MEAN_RATING = \"author_mean_rating\"\n",
    "F_BOOK_GENRES_COUNT = \"book_genres_count\"\n",
    "\n",
    "# –ù–æ–≤—ã–µ —Ñ–∏—á–∏ –∏–∑ has_read=0 (–°–¢–†–ê–¢–ï–ì–ò–Ø 2)\n",
    "F_USER_TOREAD_COUNT = \"user_toread_count\"\n",
    "F_USER_READ_TOREAD_RATIO = \"user_read_toread_ratio\"\n",
    "F_BOOK_TOREAD_COUNT = \"book_toread_count\"\n",
    "F_BOOK_READ_TOREAD_RATIO = \"book_read_toread_ratio\"\n",
    "F_IS_IN_TOREAD_LIST = \"is_in_toread_list\"\n",
    "F_DAYS_SINCE_LAST_TOREAD = \"days_since_last_toread\"\n",
    "F_USER_TOREAD_ACTIVITY = \"user_toread_activity\"\n",
    "F_BOOK_TOREAD_POPULARITY = \"book_toread_popularity\"\n",
    "\n",
    "# ALS —Ñ–∏—á–∏\n",
    "F_ALS_USER_SCORE = \"als_user_score\"\n",
    "F_ALS_BOOK_SCORE = \"als_book_score\"\n",
    "F_ALS_DOT_PRODUCT = \"als_dot_product\"\n",
    "\n",
    "# –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "COL_GENDER = \"gender\"\n",
    "COL_AGE = \"age\"\n",
    "COL_AUTHOR_ID = \"author_id\"\n",
    "COL_PUBLICATION_YEAR = \"publication_year\"\n",
    "COL_LANGUAGE = \"language\"\n",
    "COL_PUBLISHER = \"publisher\"\n",
    "COL_AVG_RATING = \"avg_rating\"\n",
    "COL_GENRE_ID = \"genre_id\"\n",
    "COL_DESCRIPTION = \"description\"\n",
    "\n",
    "# --- –ó–ù–ê–ß–ï–ù–ò–Ø ---\n",
    "VAL_SOURCE_TRAIN = \"train\"\n",
    "VAL_SOURCE_TEST = \"test\"\n",
    "\n",
    "# --- –ú–ê–ì–ò–ß–ï–°–ö–ò–ï –ß–ò–°–õ–ê ---\n",
    "MISSING_CAT_VALUE = \"-1\"\n",
    "MISSING_NUM_VALUE = -1\n",
    "PREDICTION_MIN_VALUE = 0\n",
    "PREDICTION_MAX_VALUE = 10\n",
    "ALS_FACTORS = 32\n",
    "TOP_N_GENRES = 10\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø\n",
    "# =============================================================================\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é\n",
    "ROOT_DIR = Path(\"/home/evstigneva/nto25/baseline\").resolve()\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "RAW_DATA_DIR = Path(\"/home/evstigneva/Zagr\")\n",
    "INTERIM_DATA_DIR = DATA_DIR / \"interim\" \n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "OUTPUT_DIR = ROOT_DIR / \"output\"\n",
    "MODEL_DIR = OUTPUT_DIR / \"models\"\n",
    "SUBMISSION_DIR = OUTPUT_DIR / \"submissions\"\n",
    "ALS_DIR = MODEL_DIR / \"als\"\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "for dir_path in [DATA_DIR, PROCESSED_DATA_DIR, MODEL_DIR, SUBMISSION_DIR, ALS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ–∑–¥–∞–Ω—ã:\")\n",
    "print(f\"   - –î–∞–Ω–Ω—ã–µ: {DATA_DIR}\")\n",
    "print(f\"   - –ú–æ–¥–µ–ª–∏: {MODEL_DIR}\")\n",
    "print(f\"   - ALS –º–æ–¥–µ–ª–∏: {ALS_DIR}\")\n",
    "print(f\"   - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {SUBMISSION_DIR}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"\\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "data_files = [\n",
    "    TRAIN_FILENAME, TEST_FILENAME, USER_DATA_FILENAME, \n",
    "    BOOK_DATA_FILENAME, BOOK_GENRES_FILENAME, GENRES_FILENAME, BOOK_DESCRIPTIONS_FILENAME\n",
    "]\n",
    "\n",
    "for file_path in data_files:\n",
    "    if Path(file_path).exists():\n",
    "        print(f\"   ‚úÖ {Path(file_path).name} - –Ω–∞–π–¥–µ–Ω\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {Path(file_path).name} - –ù–ï –ù–ê–ô–î–ï–ù\")\n",
    "\n",
    "# --- –ü–ê–†–ê–ú–ï–¢–†–´ ---\n",
    "RANDOM_STATE = 42\n",
    "TARGET = COL_TARGET\n",
    "\n",
    "# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –í–†–ï–ú–ï–ù–ù–û–ì–û –†–ê–ó–î–ï–õ–ï–ù–ò–Ø ---\n",
    "TEMPORAL_SPLIT_RATIO = 0.8\n",
    "\n",
    "# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø ---\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "MODEL_FILENAME = \"model_with_toread.pkl\"  # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–º—è\n",
    "\n",
    "# --- –ü–ê–†–ê–ú–ï–¢–†–´ TF-IDF ---\n",
    "TFIDF_MAX_FEATURES = 100  # –£–º–µ–Ω—å—à–∏–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "TFIDF_MIN_DF = 2\n",
    "TFIDF_MAX_DF = 0.95\n",
    "TFIDF_NGRAM_RANGE = (1, 1)\n",
    "\n",
    "# --- –ü–ê–†–ê–ú–ï–¢–†–´ ALS ---\n",
    "ALS_REGULARIZATION = 0.1\n",
    "ALS_ITERATIONS = 10\n",
    "\n",
    "# --- –§–ò–ß–ò ---\n",
    "CAT_FEATURES = [\n",
    "    COL_USER_ID,\n",
    "    COL_BOOK_ID,\n",
    "    COL_GENDER,\n",
    "    COL_AUTHOR_ID,\n",
    "    COL_LANGUAGE,\n",
    "    COL_PUBLISHER,\n",
    "    F_IS_IN_TOREAD_LIST,\n",
    "]\n",
    "\n",
    "# --- –ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ò ---\n",
    "LGB_PARAMS = {\n",
    "    \"objective\": \"rmse\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"n_estimators\": 500,  # –£–º–µ–Ω—å—à–∏–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"lambda_l1\": 0.1,\n",
    "    \"lambda_l2\": 0.1,\n",
    "    \"num_leaves\": 31,\n",
    "    \"min_child_samples\": 20,\n",
    "    \"verbose\": -1,\n",
    "    \"n_jobs\": -1,\n",
    "    \"seed\": RANDOM_STATE,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "}\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è RandomForest (–µ—Å–ª–∏ LightGBM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)\n",
    "RF_PARAMS = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 20,\n",
    "    \"min_samples_split\": 20,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"n_jobs\": -1,\n",
    "    \"verbose\": 0\n",
    "}\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò\n",
    "# =============================================================================\n",
    "\n",
    "def print_memory_usage(df: pd.DataFrame, name: str = \"DataFrame\"):\n",
    "    \"\"\"–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏.\"\"\"\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / 1024 ** 2\n",
    "    print(f\"{name}: {memory_mb:.2f} MB, {len(df):,} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫\")\n",
    "\n",
    "def optimize_dataframe_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        if col in [COL_USER_ID, COL_BOOK_ID, COL_AUTHOR_ID]:\n",
    "            df[col] = df[col].astype('int32')\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "    for col in CAT_FEATURES:\n",
    "        if col in df.columns and df[col].nunique() < 0.5 * len(df):\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –§–£–ù–ö–¶–ò–ò –û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–•\n",
    "# =============================================================================\n",
    "\n",
    "def load_all_interactions() -> pd.DataFrame:\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –í–°–ï –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏–∑ train.csv (–∏ has_read=1, –∏ has_read=0).\"\"\"\n",
    "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∏–∑ train.csv...\")\n",
    "    \n",
    "    dtype_spec = {\n",
    "        COL_USER_ID: \"int32\",\n",
    "        COL_BOOK_ID: \"int32\",\n",
    "        COL_HAS_READ: \"int8\",\n",
    "        COL_TARGET: \"float32\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        all_interactions = pd.read_csv(\n",
    "            TRAIN_FILENAME,\n",
    "            dtype=dtype_spec,\n",
    "            parse_dates=[COL_TIMESTAMP],\n",
    "        )\n",
    "        \n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_interactions):,} –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\")\n",
    "        print(f\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ has_read: {all_interactions[COL_HAS_READ].value_counts().to_dict()}\")\n",
    "        \n",
    "        return all_interactions\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_and_merge_data():\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö –≤ –µ–¥–∏–Ω—ã–π DataFrame.\"\"\"\n",
    "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "    dtype_spec = {\n",
    "        COL_USER_ID: \"int32\",\n",
    "        COL_BOOK_ID: \"int32\",\n",
    "        COL_TARGET: \"float32\",\n",
    "        COL_GENDER: \"category\",\n",
    "        COL_AGE: \"float32\",\n",
    "        COL_AUTHOR_ID: \"int32\",\n",
    "        COL_PUBLICATION_YEAR: \"float32\",\n",
    "        COL_LANGUAGE: \"category\",\n",
    "        COL_PUBLISHER: \"category\",\n",
    "        COL_AVG_RATING: \"float32\",\n",
    "        COL_GENRE_ID: \"int16\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
    "        train_df = pd.read_csv(\n",
    "            TRAIN_FILENAME,\n",
    "            dtype={\n",
    "                k: v\n",
    "                for k, v in dtype_spec.items()\n",
    "                if k in [COL_USER_ID, COL_BOOK_ID, COL_TARGET]\n",
    "            },\n",
    "            parse_dates=[COL_TIMESTAMP],\n",
    "        )\n",
    "\n",
    "        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: —Ç–æ–ª—å–∫–æ –∫–Ω–∏–≥–∏ —Å —Ä–µ–π—Ç–∏–Ω–≥–æ–º (has_read=1) –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò\n",
    "        initial_count = len(train_df)\n",
    "        train_df_filtered = train_df[train_df[COL_HAS_READ] == 1].copy()\n",
    "        filtered_count = len(train_df_filtered)\n",
    "        print(f\"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {initial_count} -> {filtered_count} —Å—Ç—Ä–æ–∫ (—Ç–æ–ª—å–∫–æ has_read=1)\")\n",
    "        \n",
    "        test_df = pd.read_csv(\n",
    "            TEST_FILENAME,\n",
    "            dtype={k: v for k, v in dtype_spec.items() if k in [COL_USER_ID, COL_BOOK_ID]},\n",
    "        )\n",
    "        user_data_df = pd.read_csv(\n",
    "            USER_DATA_FILENAME,\n",
    "            dtype={\n",
    "                k: v for k, v in dtype_spec.items() if k in [COL_USER_ID, COL_GENDER, COL_AGE]\n",
    "            },\n",
    "        )\n",
    "        book_data_df = pd.read_csv(\n",
    "            BOOK_DATA_FILENAME,\n",
    "            dtype={\n",
    "                k: v\n",
    "                for k, v in dtype_spec.items()\n",
    "                if k\n",
    "                in [\n",
    "                    COL_BOOK_ID,\n",
    "                    COL_AUTHOR_ID,\n",
    "                    COL_PUBLICATION_YEAR,\n",
    "                    COL_LANGUAGE,\n",
    "                    COL_AVG_RATING,\n",
    "                    COL_PUBLISHER,\n",
    "                ]\n",
    "            },\n",
    "        )\n",
    "        book_genres_df = pd.read_csv(\n",
    "            BOOK_GENRES_FILENAME,\n",
    "            dtype={k: v for k, v in dtype_spec.items() if k in [COL_BOOK_ID, COL_GENRE_ID]},\n",
    "        )\n",
    "        genres_df = pd.read_csv(GENRES_FILENAME)\n",
    "        book_descriptions_df = pd.read_csv(\n",
    "            BOOK_DESCRIPTIONS_FILENAME,\n",
    "            dtype={COL_BOOK_ID: \"int32\"},\n",
    "        )\n",
    "\n",
    "        print(\"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\")\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º train (—Ç–æ–ª—å–∫–æ has_read=1) –∏ test\n",
    "        train_df_filtered[COL_SOURCE] = VAL_SOURCE_TRAIN\n",
    "        test_df[COL_SOURCE] = VAL_SOURCE_TEST\n",
    "        combined_df = pd.concat([train_df_filtered, test_df], ignore_index=True, sort=False)\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "        combined_df = combined_df.merge(user_data_df, on=COL_USER_ID, how=\"left\")\n",
    "\n",
    "        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ book_data_df –ø–µ—Ä–µ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º\n",
    "        book_data_df = book_data_df.drop_duplicates(subset=[COL_BOOK_ID])\n",
    "        combined_df = combined_df.merge(book_data_df, on=COL_BOOK_ID, how=\"left\")\n",
    "\n",
    "        print(f\"–†–∞–∑–º–µ—Ä –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {combined_df.shape}\")\n",
    "        \n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–∞–∫–∂–µ –≤—Å–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "        return combined_df, book_genres_df, book_descriptions_df, train_df_filtered\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –§–£–ù–ö–¶–ò–ò –î–õ–Ø –°–¢–†–ê–¢–ï–ì–ò–ò 2 (has_read=0) - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø\n",
    "# =============================================================================\n",
    "\n",
    "def create_toread_basic_features(df: pd.DataFrame, all_interactions: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ '–Ω–∞ –ø—Ä–æ—á—Ç–µ–Ω–∏–µ' (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è).\"\"\"\n",
    "    print(\"–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —Ñ–∏—á –∏–∑ —Å–ø–∏—Å–∫–∞ '–Ω–∞ –ø—Ä–æ—á—Ç–µ–Ω–∏–µ'...\")\n",
    "    \n",
    "    if all_interactions.empty:\n",
    "        print(\"  ‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏—á.\")\n",
    "        return df\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. –§–ª–∞–≥: –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –∫–Ω–∏–≥–∞ –≤ —Å–ø–∏—Å–∫–µ \"–Ω–∞ –ø—Ä–æ—á—Ç–µ–Ω–∏–µ\" —É –¥–∞–Ω–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    print(\"  - –§–ª–∞–≥ is_in_toread_list...\")\n",
    "    \n",
    "    try:\n",
    "        # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–∞—Ä (user_id, book_id) –∏–∑ —Å–ø–∏—Å–∫–∞ \"–Ω–∞ –ø—Ä–æ—á—Ç–µ–Ω–∏–µ\"\n",
    "        toread_interactions = all_interactions[all_interactions[COL_HAS_READ] == 0]\n",
    "        \n",
    "        if not toread_interactions.empty:\n",
    "            toread_pairs = set(zip(toread_interactions[COL_USER_ID], toread_interactions[COL_BOOK_ID]))\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–µ\n",
    "            df[F_IS_IN_TOREAD_LIST] = df.apply(\n",
    "                lambda row: 1 if (row[COL_USER_ID], row[COL_BOOK_ID]) in toread_pairs else 0, \n",
    "                axis=1\n",
    "            )\n",
    "        else:\n",
    "            df[F_IS_IN_TOREAD_LIST] = 0\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–ª–∞–≥–∞ is_in_toread_list: {e}\")\n",
    "        df[F_IS_IN_TOREAD_LIST] = 0\n",
    "    \n",
    "    # 2. –ü—Ä–æ—Å—Ç—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º\n",
    "    print(\"  - –ü—Ä–æ—Å—Ç—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...\")\n",
    "    \n",
    "    try:\n",
    "        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–Ω–∏–≥ –≤ —Å–ø–∏—Å–∫–µ \"–Ω–∞ –ø—Ä–æ—á—Ç–µ–Ω–∏–µ\" –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "        if not toread_interactions.empty:\n",
    "            user_toread_count = toread_interactions.groupby(COL_USER_ID)[COL_BOOK_ID].count().reset_index()\n",
    "            user_toread_count.columns = [COL_USER_ID, F_USER_TOREAD_COUNT]\n",
    "            df = df.merge(user_toread_count, on=COL_USER_ID, how=\"left\")\n",
    "        else:\n",
    "            df[F_USER_TOREAD_COUNT] = 0\n",
    "        \n",
    "        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "        read_interactions = all_interactions[all_interactions[COL_HAS_READ] == 1]\n",
    "        if not read_interactions.empty:\n",
    "            user_read_count = read_interactions.groupby(COL_USER_ID)[COL_BOOK_ID].count().reset_index()\n",
    "            user_read_count.columns = [COL_USER_ID, 'user_read_count_temp']\n",
    "            df = df.merge(user_read_count, on=COL_USER_ID, how=\"left\")\n",
    "        else:\n",
    "            df['user_read_count_temp'] = 0\n",
    "        \n",
    "        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã—Ö –∫ –∫–Ω–∏–≥–∞–º –≤ —Å–ø–∏—Å–∫–µ\n",
    "        df[F_USER_READ_TOREAD_RATIO] = df.apply(\n",
    "            lambda x: x['user_read_count_temp'] / max(x.get(F_USER_TOREAD_COUNT, 1), 1),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É\n",
    "        if 'user_read_count_temp' in df.columns:\n",
    "            df = df.drop(columns=['user_read_count_temp'])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {e}\")\n",
    "    \n",
    "    # 3. –ü—Ä–æ—Å—Ç—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–Ω–∏–≥–∞–º\n",
    "    try:\n",
    "        if not toread_interactions.empty:\n",
    "            book_toread_count = toread_interactions.groupby(COL_BOOK_ID)[COL_USER_ID].count().reset_index()\n",
    "            book_toread_count.columns = [COL_BOOK_ID, F_BOOK_TOREAD_COUNT]\n",
    "            df = df.merge(book_toread_count, on=COL_BOOK_ID, how=\"left\")\n",
    "        else:\n",
    "            df[F_BOOK_TOREAD_COUNT] = 0\n",
    "        \n",
    "        if not read_interactions.empty:\n",
    "            book_read_count = read_interactions.groupby(COL_BOOK_ID)[COL_USER_ID].count().reset_index()\n",
    "            book_read_count.columns = [COL_BOOK_ID, 'book_read_count_temp']\n",
    "            df = df.merge(book_read_count, on=COL_BOOK_ID, how=\"left\")\n",
    "        else:\n",
    "            df['book_read_count_temp'] = 0\n",
    "        \n",
    "        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–æ—á—Ç–µ–Ω–∏–π –∫ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è–º –≤ —Å–ø–∏—Å–æ–∫\n",
    "        df[F_BOOK_READ_TOREAD_RATIO] = df.apply(\n",
    "            lambda x: x['book_read_count_temp'] / max(x.get(F_BOOK_TOREAD_COUNT, 1), 1),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É\n",
    "        if 'book_read_count_temp' in df.columns:\n",
    "            df = df.drop(columns=['book_read_count_temp'])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∫–Ω–∏–≥: {e}\")\n",
    "    \n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
    "    fill_values = {\n",
    "        F_USER_TOREAD_COUNT: 0,\n",
    "        F_USER_READ_TOREAD_RATIO: 0,\n",
    "        F_BOOK_TOREAD_COUNT: 0,\n",
    "        F_BOOK_READ_TOREAD_RATIO: 0,\n",
    "        F_IS_IN_TOREAD_LIST: 0,\n",
    "    }\n",
    "    \n",
    "    for col, fill_value in fill_values.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(fill_value)\n",
    "    \n",
    "    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏\n",
    "    added_features = sum(1 for col in df.columns if any(keyword in col.lower() for keyword in ['toread', 'is_in']))\n",
    "    print(f\"  –î–æ–±–∞–≤–ª–µ–Ω–æ {added_features} —Ñ–∏—á –∏–∑ —Å–ø–∏—Å–∫–∞ '–Ω–∞ –ø—Ä–æ—á—Ç–µ–Ω–∏–µ'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_simple_als_features(df: pd.DataFrame, all_interactions: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è ALS —Ñ–∏—á–µ–π.\"\"\"\n",
    "    if not ALS_AVAILABLE or all_interactions.empty:\n",
    "        print(\"  ‚ö†Ô∏è  ALS —Ñ–∏—á–∏ –æ—Ç–∫–ª—é—á–µ–Ω—ã –∏–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö\")\n",
    "        return df\n",
    "    \n",
    "    print(\"–°–æ–∑–¥–∞–Ω–∏–µ —É–ø—Ä–æ—â–µ–Ω–Ω—ã—Ö ALS —Ñ–∏—á–µ–π...\")\n",
    "    \n",
    "    try:\n",
    "        # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Å—á–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\n",
    "        user_item_counts = all_interactions.groupby([COL_USER_ID, COL_BOOK_ID]).size().reset_index(name='interaction_count')\n",
    "        \n",
    "        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å –∫–Ω–∏–≥–∞–º–∏\n",
    "        user_avg_interactions = user_item_counts.groupby(COL_USER_ID)['interaction_count'].mean().reset_index()\n",
    "        user_avg_interactions.columns = [COL_USER_ID, F_ALS_USER_SCORE]\n",
    "        \n",
    "        # –î–ª—è –∫–∞–∂–¥–æ–π –∫–Ω–∏–≥–∏: —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏\n",
    "        book_avg_interactions = user_item_counts.groupby(COL_BOOK_ID)['interaction_count'].mean().reset_index()\n",
    "        book_avg_interactions.columns = [COL_BOOK_ID, F_ALS_BOOK_SCORE]\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Å–Ω–æ–≤–Ω—ã–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–º\n",
    "        df = df.merge(user_avg_interactions, on=COL_USER_ID, how=\"left\")\n",
    "        df = df.merge(book_avg_interactions, on=COL_BOOK_ID, how=\"left\")\n",
    "        \n",
    "        # –°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ)\n",
    "        df[F_ALS_DOT_PRODUCT] = df[F_ALS_USER_SCORE] * df[F_ALS_BOOK_SCORE]\n",
    "        \n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
    "        for col in [F_ALS_USER_SCORE, F_ALS_BOOK_SCORE, F_ALS_DOT_PRODUCT]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(0)\n",
    "        \n",
    "        print(\"  –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ ALS —Ñ–∏—á–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è ALS —Ñ–∏—á–µ–π: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –§–ò–ß–ï–ô\n",
    "# =============================================================================\n",
    "\n",
    "def add_aggregate_features(df, train_df, all_interactions):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –∫–Ω–∏–≥ –∏ –∞–≤—Ç–æ—Ä–æ–≤.\"\"\"\n",
    "    print(\"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á...\")\n",
    "\n",
    "    if train_df.empty or all_interactions.empty:\n",
    "        print(\"  ‚ö†Ô∏è  –ù–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–≥—Ä–µ–≥–∞—Ç—ã.\")\n",
    "        return df\n",
    "\n",
    "    try:\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ has_read=1 –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ç–æ–≤\n",
    "        read_interactions = all_interactions[all_interactions[COL_HAS_READ] == 1]\n",
    "        \n",
    "        if read_interactions.empty:\n",
    "            print(\"  ‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö —Å has_read=1. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–≥—Ä–µ–≥–∞—Ç—ã.\")\n",
    "            return df\n",
    "        \n",
    "        # –ê–≥—Ä–µ–≥–∞—Ç—ã –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º (—Ç–æ–ª—å–∫–æ has_read=1)\n",
    "        user_agg = read_interactions.groupby(COL_USER_ID)[TARGET].agg([\"mean\", \"count\"]).reset_index()\n",
    "        user_agg.columns = [\n",
    "            COL_USER_ID,\n",
    "            F_USER_MEAN_RATING,\n",
    "            F_USER_RATINGS_COUNT,\n",
    "        ]\n",
    "\n",
    "        # –ê–≥—Ä–µ–≥–∞—Ç—ã –ø–æ –∫–Ω–∏–≥–∞–º (—Ç–æ–ª—å–∫–æ has_read=1)\n",
    "        book_agg = read_interactions.groupby(COL_BOOK_ID)[TARGET].agg([\"mean\", \"count\"]).reset_index()\n",
    "        book_agg.columns = [\n",
    "            COL_BOOK_ID,\n",
    "            F_BOOK_MEAN_RATING,\n",
    "            F_BOOK_RATINGS_COUNT,\n",
    "        ]\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∞–≥—Ä–µ–≥–∞—Ç—ã —Å –æ—Å–Ω–æ–≤–Ω—ã–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–º\n",
    "        df = df.merge(user_agg, on=COL_USER_ID, how=\"left\")\n",
    "        df = df.merge(book_agg, on=COL_BOOK_ID, how=\"left\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥—Ä–µ–≥–∞—Ç–Ω—ã—Ö —Ñ–∏—á: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_genre_features(df, book_genres_df):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∂–∞–Ω—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–Ω–∏–≥–∏.\"\"\"\n",
    "    print(\"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏—á –∂–∞–Ω—Ä–æ–≤...\")\n",
    "    \n",
    "    if book_genres_df.empty:\n",
    "        print(\"  ‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –∂–∞–Ω—Ä–∞—Ö. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.\")\n",
    "        return df\n",
    "    \n",
    "    try:\n",
    "        genre_counts = book_genres_df.groupby(COL_BOOK_ID)[COL_GENRE_ID].count().reset_index()\n",
    "        genre_counts.columns = [\n",
    "            COL_BOOK_ID,\n",
    "            F_BOOK_GENRES_COUNT,\n",
    "        ]\n",
    "        df = df.merge(genre_counts, on=COL_BOOK_ID, how=\"left\")\n",
    "        df[F_BOOK_GENRES_COUNT] = df[F_BOOK_GENRES_COUNT].fillna(0)\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∂–∞–Ω—Ä–æ–≤—ã—Ö —Ñ–∏—á: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_text_features(df, train_df, descriptions_df):\n",
    "    \"\"\"–î–æ–±–∞–≤–ª—è–µ—Ç TF-IDF —Ñ–∏—á–∏ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏–π –∫–Ω–∏–≥.\"\"\"\n",
    "    print(\"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∏—á (TF-IDF)...\")\n",
    "\n",
    "    if descriptions_df.empty:\n",
    "        print(\"  ‚ö†Ô∏è  –ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–π –∫–Ω–∏–≥. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º TF-IDF.\")\n",
    "        return df\n",
    "\n",
    "    vectorizer_path = MODEL_DIR / TFIDF_VECTORIZER_FILENAME\n",
    "\n",
    "    try:\n",
    "        # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–Ω–∏–≥–∏ –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n",
    "        train_books = train_df[COL_BOOK_ID].unique()\n",
    "\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∫–Ω–∏–≥\n",
    "        train_descriptions = descriptions_df[descriptions_df[COL_BOOK_ID].isin(train_books)].copy()\n",
    "        train_descriptions[COL_DESCRIPTION] = train_descriptions[COL_DESCRIPTION].fillna(\"\")\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä–∞\n",
    "        if vectorizer_path.exists():\n",
    "            print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä–∞ –∏–∑ {vectorizer_path}\")\n",
    "            vectorizer = joblib.load(vectorizer_path)\n",
    "        else:\n",
    "            # –û–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏—è—Ö\n",
    "            print(\"–û–±—É—á–µ–Ω–∏–µ TF-IDF –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏—è—Ö...\")\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                max_features=TFIDF_MAX_FEATURES,\n",
    "                min_df=TFIDF_MIN_DF,\n",
    "                max_df=TFIDF_MAX_DF,\n",
    "                ngram_range=TFIDF_NGRAM_RANGE,\n",
    "            )\n",
    "            vectorizer.fit(train_descriptions[COL_DESCRIPTION])\n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏\n",
    "            joblib.dump(vectorizer, vectorizer_path)\n",
    "            print(f\"–í–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {vectorizer_path}\")\n",
    "\n",
    "        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –≤—Å–µ –æ–ø–∏—Å–∞–Ω–∏—è –∫–Ω–∏–≥\n",
    "        all_descriptions = descriptions_df[[COL_BOOK_ID, COL_DESCRIPTION]].copy()\n",
    "        all_descriptions[COL_DESCRIPTION] = all_descriptions[COL_DESCRIPTION].fillna(\"\")\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ book_id -> description\n",
    "        description_map = dict(\n",
    "            zip(all_descriptions[COL_BOOK_ID], all_descriptions[COL_DESCRIPTION])\n",
    "        )\n",
    "\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –∫–Ω–∏–≥ –≤ df\n",
    "        df_descriptions = df[COL_BOOK_ID].map(description_map).fillna(\"\")\n",
    "\n",
    "        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –≤ TF-IDF —Ñ–∏—á–∏\n",
    "        tfidf_matrix = vectorizer.transform(df_descriptions)\n",
    "\n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –≤ DataFrame\n",
    "        tfidf_feature_names = [f\"tfidf_{i}\" for i in range(tfidf_matrix.shape[1])]\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_matrix.toarray(),\n",
    "            columns=tfidf_feature_names,\n",
    "            index=df.index,\n",
    "        )\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º TF-IDF —Ñ–∏—á–∏ —Å –æ—Å–Ω–æ–≤–Ω—ã–º DataFrame\n",
    "        df_with_tfidf = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        print(f\"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(tfidf_feature_names)} TF-IDF —Ñ–∏—á.\")\n",
    "        return df_with_tfidf\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è TF-IDF —Ñ–∏—á: {e}\")\n",
    "        return df\n",
    "\n",
    "def handle_missing_values(df, train_df):\n",
    "    \"\"\"–ó–∞–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.\"\"\"\n",
    "    print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...\")\n",
    "\n",
    "    try:\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "        if not train_df.empty and TARGET in train_df.columns:\n",
    "            global_mean = train_df[TARGET].mean()\n",
    "        else:\n",
    "            global_mean = 5.0\n",
    "\n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç –º–µ–¥–∏–∞–Ω–æ–π\n",
    "        if COL_AGE in df.columns:\n",
    "            age_median = df[COL_AGE].median()\n",
    "            df[COL_AGE] = df[COL_AGE].fillna(age_median)\n",
    "\n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏ –¥–ª—è \"—Ö–æ–ª–æ–¥–Ω–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞\"\n",
    "        for col in [F_USER_MEAN_RATING, F_BOOK_MEAN_RATING, COL_AVG_RATING]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(global_mean)\n",
    "\n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –Ω—É–ª—è–º–∏\n",
    "        count_cols = [\n",
    "            F_USER_RATINGS_COUNT, F_BOOK_RATINGS_COUNT,\n",
    "            F_USER_TOREAD_COUNT, F_BOOK_TOREAD_COUNT,\n",
    "            F_BOOK_GENRES_COUNT\n",
    "        ]\n",
    "        for col in count_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(0)\n",
    "\n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –Ω—É–ª—è–º–∏\n",
    "        ratio_cols = [\n",
    "            F_USER_READ_TOREAD_RATIO, F_BOOK_READ_TOREAD_RATIO,\n",
    "        ]\n",
    "        for col in ratio_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(0)\n",
    "\n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–∏—á–∏\n",
    "        if F_IS_IN_TOREAD_LIST in df.columns:\n",
    "            df[F_IS_IN_TOREAD_LIST] = df[F_IS_IN_TOREAD_LIST].fillna(0).astype(int)\n",
    "\n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º ALS —Ñ–∏—á–∏ –Ω—É–ª—è–º–∏\n",
    "        als_cols = [F_ALS_USER_SCORE, F_ALS_BOOK_SCORE, F_ALS_DOT_PRODUCT]\n",
    "        for col in als_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(0.0)\n",
    "\n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º TF-IDF —Ñ–∏—á–∏ –Ω—É–ª—è–º–∏\n",
    "        tfidf_cols = [col for col in df.columns if col.startswith(\"tfidf_\")]\n",
    "        for col in tfidf_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(0.0)\n",
    "\n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
    "        for col in df.columns:\n",
    "            if col in CAT_FEATURES:\n",
    "                if df[col].dtype.name in (\"category\", \"object\") and df[col].isna().any():\n",
    "                    df[col] = df[col].astype(str).fillna(MISSING_CAT_VALUE).astype(\"category\")\n",
    "                elif pd.api.types.is_numeric_dtype(df[col].dtype) and df[col].isna().any():\n",
    "                    df[col] = df[col].fillna(MISSING_NUM_VALUE)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_features(df, all_interactions, book_genres_df, descriptions_df, train_df):\n",
    "    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ —Ñ–∏—á (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è).\"\"\"\n",
    "    print(\"–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ —Ñ–∏—á...\")\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"  ‚ö†Ô∏è  –ü—É—Å—Ç–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–Ω–∂–µ–Ω–µ—Ä–∏—é —Ñ–∏—á.\")\n",
    "        return df\n",
    "    \n",
    "    # 1. –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏\n",
    "    df = add_genre_features(df, book_genres_df)\n",
    "    \n",
    "    # 2. –§–∏—á–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ \"–Ω–∞ –ø—Ä–æ—á—Ç–µ–Ω–∏–µ\" (–°–¢–†–ê–¢–ï–ì–ò–Ø 2)\n",
    "    df = create_toread_basic_features(df, all_interactions)\n",
    "    \n",
    "    # 3. ALS —Ñ–∏—á–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ)\n",
    "    df = create_simple_als_features(df, all_interactions)\n",
    "    \n",
    "    # 4. –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∏—á–∏\n",
    "    df = add_text_features(df, train_df, descriptions_df)\n",
    "    \n",
    "    # 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "    df = handle_missing_values(df, train_df)\n",
    "\n",
    "    print(\"–ò–Ω–∂–µ–Ω–µ—Ä–∏—è —Ñ–∏—á –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")\n",
    "    return df\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –§–£–ù–ö–¶–ò–ò –í–†–ï–ú–ï–ù–ù–û–ì–û –†–ê–ó–î–ï–õ–ï–ù–ò–Ø\n",
    "# =============================================================================\n",
    "\n",
    "def temporal_split_by_date(df, split_date, timestamp_col=COL_TIMESTAMP):\n",
    "    \"\"\"–†–∞–∑–¥–µ–ª—è–µ—Ç DataFrame –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏ –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π –¥–∞—Ç–µ.\"\"\"\n",
    "    if timestamp_col not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ '{timestamp_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ.\")\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        mask = np.random.rand(len(df)) < TEMPORAL_SPLIT_RATIO\n",
    "        return pd.Series(mask), pd.Series(~mask)\n",
    "\n",
    "    try:\n",
    "        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –≤ datetime —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):\n",
    "            df = df.copy()\n",
    "            df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "\n",
    "        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –ø–æ—Ä–æ–≥—É –¥–∞—Ç—ã\n",
    "        train_mask = df[timestamp_col] <= split_date\n",
    "        val_mask = df[timestamp_col] > split_date\n",
    "\n",
    "        # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "        if train_mask.sum() == 0:\n",
    "            print(f\"‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π <= {split_date}.\")\n",
    "            return pd.Series([True] * len(df)), pd.Series([False] * len(df))\n",
    "\n",
    "        if val_mask.sum() == 0:\n",
    "            print(f\"‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–∞ > {split_date}.\")\n",
    "            return pd.Series([True] * len(df)), pd.Series([False] * len(df))\n",
    "\n",
    "        return train_mask, val_mask\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: {e}\")\n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        mask = np.random.rand(len(df)) < TEMPORAL_SPLIT_RATIO\n",
    "        return pd.Series(mask), pd.Series(~mask)\n",
    "\n",
    "def get_split_date_from_ratio(df, ratio, timestamp_col=COL_TIMESTAMP):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –¥–∞—Ç—É —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö.\"\"\"\n",
    "    if not 0 < ratio < 1:\n",
    "        ratio = 0.8  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "\n",
    "    if timestamp_col not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ '{timestamp_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –≤ datetime —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):\n",
    "            df = df.copy()\n",
    "            df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è\n",
    "        sorted_timestamps = df[timestamp_col].sort_values()\n",
    "        threshold_index = int(len(sorted_timestamps) * ratio)\n",
    "\n",
    "        return sorted_timestamps.iloc[threshold_index]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–∞—Ç—ã —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: {e}\")\n",
    "        return None\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –ü–ê–ô–ü–õ–ê–ô–ù–ê\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –≤ processed –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"–ü–ê–ô–ü–õ–ê–ô–ù –ü–û–î–ì–û–¢–û–í–ö–ò –î–ê–ù–ù–´–• –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤\n",
    "    required_files = [TRAIN_FILENAME, TEST_FILENAME]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not Path(f).exists()]\n",
    "    if missing_files:\n",
    "        print(f\"‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã: {missing_files}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    merged_df, book_genres_df, descriptions_df, train_df_filtered = load_and_merge_data()\n",
    "    \n",
    "    if merged_df.empty:\n",
    "        print(\"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∏—á–µ–π\n",
    "    print(\"\\n–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∏—á–µ–π...\")\n",
    "    all_interactions = load_all_interactions()\n",
    "\n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–Ω–∂–µ–Ω–µ—Ä–∏—é —Ñ–∏—á\n",
    "    print(\"\\n–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ —Ñ–∏—á...\")\n",
    "    featured_df = create_features(\n",
    "        merged_df, \n",
    "        all_interactions, \n",
    "        book_genres_df, \n",
    "        descriptions_df,\n",
    "        train_df_filtered\n",
    "    )\n",
    "\n",
    "    # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏\n",
    "    featured_df = optimize_dataframe_dtypes(featured_df)\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –≤—ã–≤–æ–¥–∞\n",
    "    processed_path = PROCESSED_DATA_DIR / PROCESSED_DATA_FILENAME\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ parquet –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\n",
    "    print(f\"\\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ {processed_path}...\")\n",
    "    try:\n",
    "        featured_df.to_parquet(processed_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "        print(\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}\")\n",
    "        # –ü—Ä–æ–±—É–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ CSV\n",
    "        csv_path = processed_path.with_suffix('.csv')\n",
    "        featured_df.to_csv(csv_path, index=False)\n",
    "        print(f\"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∫–∞–∫ CSV: {csv_path}\")\n",
    "\n",
    "    # –ü–µ—á–∞—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
    "    if not featured_df.empty:\n",
    "        train_rows = len(featured_df[featured_df[COL_SOURCE] == VAL_SOURCE_TRAIN])\n",
    "        test_rows = len(featured_df[featured_df[COL_SOURCE] == VAL_SOURCE_TEST])\n",
    "        total_features = len(featured_df.columns)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"–ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù–ê!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"  - –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏: {train_rows:,}\")\n",
    "        print(f\"  - –¢–µ—Å—Ç–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏: {test_rows:,}\")\n",
    "        print(f\"  - –í—Å–µ–≥–æ —Ñ–∏—á: {total_features}\")\n",
    "        print(f\"  - –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {processed_path}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã.\")\n",
    "    \n",
    "    return featured_df\n",
    "\n",
    "def train():\n",
    "    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    processed_path = PROCESSED_DATA_DIR / PROCESSED_DATA_FILENAME\n",
    "\n",
    "    if not processed_path.exists():\n",
    "        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ CSV –≤–µ—Ä—Å–∏—é\n",
    "        csv_path = processed_path.with_suffix('.csv')\n",
    "        if csv_path.exists():\n",
    "            print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ {csv_path}...\")\n",
    "            featured_df = pd.read_csv(csv_path)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ó–∞–ø—É—Å–∫–∞–µ–º prepare_data()...\")\n",
    "            featured_df = prepare_data()\n",
    "            if featured_df.empty:\n",
    "                print(\"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ.\")\n",
    "                return None, 0, 0, pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ {processed_path}...\")\n",
    "        featured_df = pd.read_parquet(processed_path, engine=\"pyarrow\")\n",
    "    \n",
    "    if featured_df.empty:\n",
    "        print(\"‚ö†Ô∏è  –ó–∞–≥—Ä—É–∂–µ–Ω –ø—É—Å—Ç–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ.\")\n",
    "        return None, 0, 0, pd.DataFrame()\n",
    "    \n",
    "    print_memory_usage(featured_df, \"–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\")\n",
    "    print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(featured_df):,} —Å—Ç—Ä–æ–∫ —Å {len(featured_df.columns)} —Ñ–∏—á–∞–º–∏\")\n",
    "\n",
    "    # –†–∞–∑–¥–µ–ª—è–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –Ω–∞–±–æ—Ä—ã\n",
    "    train_set = featured_df[featured_df[COL_SOURCE] == VAL_SOURCE_TRAIN].copy()\n",
    "    \n",
    "    if train_set.empty:\n",
    "        print(\"‚ö†Ô∏è  –ù–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ.\")\n",
    "        return None, 0, 0, pd.DataFrame()\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫\n",
    "    if COL_TIMESTAMP not in train_set.columns:\n",
    "        print(f\"‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ '{COL_TIMESTAMP}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ.\")\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        mask = np.random.rand(len(train_set)) < TEMPORAL_SPLIT_RATIO\n",
    "        train_split = train_set[mask].copy()\n",
    "        val_split = train_set[~mask].copy()\n",
    "    else:\n",
    "        try:\n",
    "            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
    "            print(f\"\\n–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º {TEMPORAL_SPLIT_RATIO}...\")\n",
    "            split_date = get_split_date_from_ratio(train_set, TEMPORAL_SPLIT_RATIO, COL_TIMESTAMP)\n",
    "            \n",
    "            if split_date is None:\n",
    "                print(\"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –¥–∞—Ç—É —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ.\")\n",
    "                np.random.seed(RANDOM_STATE)\n",
    "                mask = np.random.rand(len(train_set)) < TEMPORAL_SPLIT_RATIO\n",
    "                train_split = train_set[mask].copy()\n",
    "                val_split = train_set[~mask].copy()\n",
    "            else:\n",
    "                print(f\"–î–∞—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: {split_date}\")\n",
    "\n",
    "                train_mask, val_mask = temporal_split_by_date(train_set, split_date, COL_TIMESTAMP)\n",
    "\n",
    "                # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "                train_split = train_set[train_mask].copy()\n",
    "                val_split = train_set[val_mask].copy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: {e}\")\n",
    "            # –°–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
    "            np.random.seed(RANDOM_STATE)\n",
    "            mask = np.random.rand(len(train_set)) < TEMPORAL_SPLIT_RATIO\n",
    "            train_split = train_set[mask].copy()\n",
    "            val_split = train_set[~mask].copy()\n",
    "\n",
    "    print(f\"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(train_split):,} —Å—Ç—Ä–æ–∫\")\n",
    "    print(f\"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(val_split):,} —Å—Ç—Ä–æ–∫\")\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ–≤\n",
    "    print(\"\\n–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ–≤...\")\n",
    "    all_interactions = load_all_interactions()\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏\n",
    "    print(\"\\n–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á...\")\n",
    "    train_split_with_agg = add_aggregate_features(train_split.copy(), train_split, all_interactions)\n",
    "    val_split_with_agg = add_aggregate_features(val_split.copy(), train_split, all_interactions)\n",
    "\n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "    print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...\")\n",
    "    train_split_final = handle_missing_values(train_split_with_agg, train_split)\n",
    "    val_split_final = handle_missing_values(val_split_with_agg, train_split)\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ (X) –∏ —Ç–∞—Ä–≥–µ—Ç (y)\n",
    "    exclude_cols = [\n",
    "        COL_SOURCE,\n",
    "        TARGET,\n",
    "        COL_PREDICTION,\n",
    "        COL_TIMESTAMP,\n",
    "        COL_HAS_READ,\n",
    "    ]\n",
    "    \n",
    "    # –ò—Å–∫–ª—é—á–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö\n",
    "    available_cols = [col for col in train_split_final.columns if col not in exclude_cols]\n",
    "    \n",
    "    # –ò—Å–∫–ª—é—á–∞–µ–º object –∫–æ–ª–æ–Ω–∫–∏\n",
    "    non_feature_object_cols = train_split_final[available_cols].select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    features = [f for f in available_cols if f not in non_feature_object_cols]\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å —Ñ–∏—á–∏\n",
    "    if not features:\n",
    "        print(\"‚ö†Ô∏è  –ù–µ—Ç —Ñ–∏—á –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.\")\n",
    "        return None, 0, 0, pd.DataFrame()\n",
    "\n",
    "    X_train = train_split_final[features]\n",
    "    y_train = train_split_final[TARGET]\n",
    "    X_val = val_split_final[features]\n",
    "    y_val = val_split_final[TARGET]\n",
    "\n",
    "    print(f\"\\n–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ —Ñ–∏—á–∏: {len(features)}\")\n",
    "    print(f\"–†–∞–∑–º–µ—Ä X_train: {X_train.shape}\")\n",
    "    print(f\"–†–∞–∑–º–µ—Ä X_val: {X_val.shape}\")\n",
    "\n",
    "    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        if LGB_AVAILABLE:\n",
    "            print(\"–ò—Å–ø–æ–ª—å–∑—É–µ–º LightGBM...\")\n",
    "            model = lgb.LGBMRegressor(**LGB_PARAMS)\n",
    "\n",
    "            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã fit —Å –∫–æ–ª–±—ç–∫–æ–º —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n",
    "            fit_params = {\"eval_metric\": \"rmse\"}\n",
    "            \n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric=fit_params[\"eval_metric\"],\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=EARLY_STOPPING_ROUNDS)],\n",
    "                verbose=100,\n",
    "            )\n",
    "        else:\n",
    "            print(\"–ò—Å–ø–æ–ª—å–∑—É–µ–º RandomForestRegressor...\")\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            model = RandomForestRegressor(**RF_PARAMS)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "        val_preds = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "        mae = mean_absolute_error(y_val, val_preds)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π RMSE: {rmse:.4f}\")\n",
    "        print(f\"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π MAE: {mae:.4f}\")\n",
    "\n",
    "        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"–¢–û–ü-20 –í–ê–ñ–ù–ï–ô–®–ò–• –§–ò–ß\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False).head(20)\n",
    "            \n",
    "            for i, row in feature_importance.iterrows():\n",
    "                print(f\"  {i+1:2d}. {row['feature'][:50]:50s} - {row['importance']:8.1f}\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç feature_importances_\")\n",
    "            feature_importance = pd.DataFrame()\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "        model_path = MODEL_DIR / MODEL_FILENAME\n",
    "        joblib.dump(model, model_path)\n",
    "        print(f\"\\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}\")\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∏—á\n",
    "        features_path = MODEL_DIR / \"model_features.pkl\"\n",
    "        joblib.dump(features, features_path)\n",
    "        print(f\"‚úÖ –°–ø–∏—Å–æ–∫ —Ñ–∏—á —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {features_path}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return model, rmse, mae, feature_importance\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}\")\n",
    "        # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç—É—é –ª–∏–Ω–µ–π–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "        try:\n",
    "            print(\"–ü—Ä–æ–±—É–µ–º LinearRegression...\")\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            val_preds = model.predict(X_val)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "            mae = mean_absolute_error(y_val, val_preds)\n",
    "            \n",
    "            print(f\"‚úÖ LinearRegression RMSE: {rmse:.4f}\")\n",
    "            print(f\"‚úÖ LinearRegression MAE: {mae:.4f}\")\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
    "            model_path = MODEL_DIR / MODEL_FILENAME\n",
    "            joblib.dump(model, model_path)\n",
    "            print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}\")\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∏—á\n",
    "            features_path = MODEL_DIR / \"model_features.pkl\"\n",
    "            joblib.dump(features, features_path)\n",
    "            \n",
    "            return model, rmse, mae, pd.DataFrame()\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è LinearRegression: {e2}\")\n",
    "            return None, 0, 0, pd.DataFrame()\n",
    "\n",
    "def predict():\n",
    "    \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"–ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    processed_path = PROCESSED_DATA_DIR / PROCESSED_DATA_FILENAME\n",
    "\n",
    "    if not processed_path.exists():\n",
    "        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ CSV –≤–µ—Ä—Å–∏—é\n",
    "        csv_path = processed_path.with_suffix('.csv')\n",
    "        if csv_path.exists():\n",
    "            print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ {csv_path}...\")\n",
    "            featured_df = pd.read_csv(csv_path)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ó–∞–ø—É—Å–∫–∞–µ–º prepare_data()...\")\n",
    "            featured_df = prepare_data()\n",
    "            if featured_df.empty:\n",
    "                print(\"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.\")\n",
    "                return pd.DataFrame(), np.array([])\n",
    "    else:\n",
    "        print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ {processed_path}...\")\n",
    "        featured_df = pd.read_parquet(processed_path, engine=\"pyarrow\")\n",
    "    \n",
    "    if featured_df.empty:\n",
    "        print(\"‚ö†Ô∏è  –ó–∞–≥—Ä—É–∂–µ–Ω –ø—É—Å—Ç–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.\")\n",
    "        return pd.DataFrame(), np.array([])\n",
    "    \n",
    "    print_memory_usage(featured_df, \"–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\")\n",
    "    print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(featured_df):,} —Å—Ç—Ä–æ–∫ —Å {len(featured_df.columns)} —Ñ–∏—á–∞–º–∏\")\n",
    "\n",
    "    # –†–∞–∑–¥–µ–ª—è–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –Ω–∞–±–æ—Ä—ã\n",
    "    train_set = featured_df[featured_df[COL_SOURCE] == VAL_SOURCE_TRAIN].copy()\n",
    "    test_set = featured_df[featured_df[COL_SOURCE] == VAL_SOURCE_TEST].copy()\n",
    "    \n",
    "    if test_set.empty:\n",
    "        print(\"‚ö†Ô∏è  –ù–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.\")\n",
    "        return pd.DataFrame(), np.array([])\n",
    "\n",
    "    print(f\"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –Ω–∞–±–æ—Ä: {len(train_set):,} —Å—Ç—Ä–æ–∫\")\n",
    "    print(f\"–¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: {len(test_set):,} —Å—Ç—Ä–æ–∫\")\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ–≤\n",
    "    print(\"\\n–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ–≤...\")\n",
    "    all_interactions = load_all_interactions()\n",
    "    \n",
    "    # –î–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    print(\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á –Ω–∞ –≤—Å–µ—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    test_set_with_agg = add_aggregate_features(test_set.copy(), train_set, all_interactions)\n",
    "\n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "    print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...\")\n",
    "    test_set_final = handle_missing_values(test_set_with_agg, train_set)\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∏—á –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
    "    features_path = MODEL_DIR / \"model_features.pkl\"\n",
    "    if not features_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  –°–ø–∏—Å–æ–∫ —Ñ–∏—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π...\")\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —Ñ–∏—á–∏ –∫—Ä–æ–º–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö\n",
    "        exclude_cols = [COL_SOURCE, TARGET, COL_PREDICTION, COL_TIMESTAMP, COL_HAS_READ]\n",
    "        features = [col for col in test_set_final.columns if col not in exclude_cols]\n",
    "        # –ò—Å–∫–ª—é—á–∞–µ–º object –∫–æ–ª–æ–Ω–∫–∏\n",
    "        non_feature_object_cols = test_set_final[features].select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        features = [f for f in features if f not in non_feature_object_cols]\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "        joblib.dump(features, features_path)\n",
    "    else:\n",
    "        features = joblib.load(features_path)\n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(features)} —Ñ–∏—á –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\")\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ñ–∏—á–∏ –µ—Å—Ç—å –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    missing_features = [f for f in features if f not in test_set_final.columns]\n",
    "    if missing_features:\n",
    "        print(f\"‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∏—á–∏: {missing_features[:5]}...\")\n",
    "        print(\"–î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Ñ–∏—á–∏ —Å –Ω—É–ª–µ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏...\")\n",
    "        for f in missing_features:\n",
    "            test_set_final[f] = 0\n",
    "\n",
    "    X_test = test_set_final[features]\n",
    "    print(f\"–§–∏—á–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {X_test.shape}\")\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "    model_path = MODEL_DIR / MODEL_FILENAME\n",
    "    if not model_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {model_path}. –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
    "        model, _, _, _ = train()\n",
    "        if model is None:\n",
    "            print(\"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å. –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...\")\n",
    "            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ)\n",
    "            train_target_mean = train_set[TARGET].mean() if not train_set.empty and TARGET in train_set.columns else 5.0\n",
    "            test_preds = np.full(len(test_set), train_target_mean)\n",
    "            clipped_preds = np.clip(test_preds, PREDICTION_MIN_VALUE, PREDICTION_MAX_VALUE)\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª submission\n",
    "            submission_df = test_set[[COL_USER_ID, COL_BOOK_ID]].copy()\n",
    "            submission_df[COL_PREDICTION] = clipped_preds\n",
    "            \n",
    "            submission_path = SUBMISSION_DIR / SUBMISSION_FILENAME\n",
    "            submission_df.to_csv(submission_path, index=False)\n",
    "            \n",
    "            print(f\"‚úÖ –ë–∞–∑–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ–∑–¥–∞–Ω—ã: {submission_path}\")\n",
    "            return submission_df, clipped_preds\n",
    "    else:\n",
    "        print(f\"\\n–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}...\")\n",
    "        model = joblib.load(model_path)\n",
    "\n",
    "    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "    print(\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\")\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    # –û–±—Ä–µ–∑–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Ä–µ–π—Ç–∏–Ω–≥–∞ [0, 10]\n",
    "    clipped_preds = np.clip(test_preds, PREDICTION_MIN_VALUE, PREDICTION_MAX_VALUE)\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª submission\n",
    "    submission_df = test_set[[COL_USER_ID, COL_BOOK_ID]].copy()\n",
    "    submission_df[COL_PREDICTION] = clipped_preds\n",
    "\n",
    "    submission_path = SUBMISSION_DIR / SUBMISSION_FILENAME\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"–ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –°–§–û–†–ú–ò–†–û–í–ê–ù–´\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ –§–∞–π–ª submission —Å–æ–∑–¥–∞–Ω –≤: {submission_path}\")\n",
    "    print(f\"‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(clipped_preds):,}\")\n",
    "    print(f\"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:\")\n",
    "    print(f\"   - Min: {clipped_preds.min():.4f}\")\n",
    "    print(f\"   - Max: {clipped_preds.max():.4f}\")\n",
    "    print(f\"   - Mean: {clipped_preds.mean():.4f}\")\n",
    "    print(f\"   - Std: {clipped_preds.std():.4f}\")\n",
    "    \n",
    "    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º\n",
    "    if len(clipped_preds) > 0:\n",
    "        bins = [0, 2, 4, 6, 8, 10]\n",
    "        hist, _ = np.histogram(clipped_preds, bins=bins)\n",
    "        print(f\"\\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º:\")\n",
    "        for i in range(len(bins)-1):\n",
    "            percent = hist[i]/len(clipped_preds)*100 if len(clipped_preds) > 0 else 0\n",
    "            print(f\"   [{bins[i]:.1f}-{bins[i+1]:.1f}): {hist[i]:,} ({percent:.1f}%)\")\n",
    "    \n",
    "    return submission_df, clipped_preds\n",
    "\n",
    "def validate():\n",
    "    \"\"\"–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ submission.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"–í–ê–õ–ò–î–ê–¶–ò–Ø –§–ê–ô–õ–ê SUBMISSION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ñ–∞–π–ª submission\n",
    "        test_df = pd.read_csv(TEST_FILENAME)\n",
    "        sub_path = SUBMISSION_DIR / SUBMISSION_FILENAME\n",
    "        \n",
    "        if not sub_path.exists():\n",
    "            print(f\"‚ùå –§–∞–π–ª submission –Ω–µ –Ω–∞–π–¥–µ–Ω: {sub_path}\")\n",
    "            return False\n",
    "            \n",
    "        sub_df = pd.read_csv(sub_path)\n",
    "\n",
    "        print(f\"–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(test_df):,} —Å—Ç—Ä–æ–∫\")\n",
    "        print(f\"Submission —Ñ–∞–π–ª: {len(sub_df):,} —Å—Ç—Ä–æ–∫\")\n",
    "\n",
    "        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É\n",
    "        if len(sub_df) != len(test_df):\n",
    "            print(f\"‚ùå –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–ª–∏–Ω—ã submission. –û–∂–∏–¥–∞–ª–æ—Å—å {len(test_df)}, –ø–æ–ª—É—á–µ–Ω–æ {len(sub_df)}.\")\n",
    "            return False\n",
    "        print(\"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –ø—Ä–æ–π–¥–µ–Ω–∞.\")\n",
    "\n",
    "        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö\n",
    "        if sub_df[COL_PREDICTION].isna().any():\n",
    "            print(f\"‚ùå –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ '{COL_PREDICTION}'.\")\n",
    "            return False\n",
    "        print(\"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–æ–π–¥–µ–Ω–∞.\")\n",
    "\n",
    "        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–∞–±–æ—Ä –ø–∞—Ä (user_id, book_id) —Å–æ–≤–ø–∞–¥–∞–µ—Ç\n",
    "        test_keys = (\n",
    "            test_df[[COL_USER_ID, COL_BOOK_ID]]\n",
    "            .copy()\n",
    "            .set_index([COL_USER_ID, COL_BOOK_ID])\n",
    "        )\n",
    "        sub_keys = (\n",
    "            sub_df[[COL_USER_ID, COL_BOOK_ID]]\n",
    "            .copy()\n",
    "            .set_index([COL_USER_ID, COL_BOOK_ID])\n",
    "        )\n",
    "\n",
    "        if not test_keys.index.equals(sub_keys.index):\n",
    "            print(\"‚ùå –ù–∞–±–æ—Ä –ø–∞—Ä (user_id, book_id) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ç–µ—Å—Ç–æ–≤—ã–º –Ω–∞–±–æ—Ä–æ–º.\")\n",
    "            return False\n",
    "        print(\"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–∞—Ä (user_id, book_id) –ø—Ä–æ–π–¥–µ–Ω–∞.\")\n",
    "\n",
    "        # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "        if not sub_df[COL_PREDICTION].between(PREDICTION_MIN_VALUE, PREDICTION_MAX_VALUE).all():\n",
    "            print(f\"‚ùå –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [{PREDICTION_MIN_VALUE}, {PREDICTION_MAX_VALUE}].\")\n",
    "            return False\n",
    "        print(\"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π [0, 10] –ø—Ä–æ–π–¥–µ–Ω–∞.\")\n",
    "\n",
    "        # 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–∞—Ä\n",
    "        if sub_df[[COL_USER_ID, COL_BOOK_ID]].duplicated().sum() > 0:\n",
    "            print(\"‚ùå –ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–∞—Ä (user_id, book_id).\")\n",
    "            return False\n",
    "        print(\"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–∞—Ä –ø—Ä–æ–π–¥–µ–Ω–∞.\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"–í–ê–õ–ò–î–ê–¶–ò–Ø –£–°–ü–ï–®–ù–ê!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"–§–∞–π–ª {sub_path.name} –∏–º–µ–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏ –≥–æ—Ç–æ–≤ –∫ –∑–∞–≥—Ä—É–∑–∫–µ.\")\n",
    "        print(f\"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {sub_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞: {e}. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}\")\n",
    "        return False\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê\n",
    "# =============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\n",
    "# –í—ã–ø–æ–ª–Ω–∏—Ç–µ —è—á–µ–π–∫–∏ –Ω–∏–∂–µ –ø–æ –ø–æ—Ä—è–¥–∫—É:\n",
    "\n",
    "# %%\n",
    "# 1. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•\n",
    "print(\"üéØ –®–ê–ì 1: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\")\n",
    "featured_df = prepare_data()\n",
    "\n",
    "# %%\n",
    "# 2. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "print(\"\\nüéØ –®–ê–ì 2: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\")\n",
    "model, rmse, mae, feature_importance = train()\n",
    "\n",
    "# %%\n",
    "# 3. –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "print(\"\\nüéØ –®–ê–ì 3: –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\")\n",
    "submission_df, predictions = predict()\n",
    "\n",
    "# %%\n",
    "# 4. –í–ê–õ–ò–î–ê–¶–ò–Ø\n",
    "print(\"\\nüéØ –®–ê–ì 4: –í–ê–õ–ò–î–ê–¶–ò–Ø\")\n",
    "validation_success = validate()\n",
    "\n",
    "# %%\n",
    "# 5. –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´\n",
    "print(\"\\nüéØ –®–ê–ì 5: –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –° –§–ò–ß–ê–ú–ò –ò–ó has_read=0\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'featured_df' in locals() and not featured_df.empty:\n",
    "    print(f\"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {len(featured_df):,} —Å—Ç—Ä–æ–∫, {len(featured_df.columns)} —Ñ–∏—á\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –ù–ï –í–´–ü–û–õ–ù–ï–ù–ê\")\n",
    "\n",
    "print(f\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: RMSE = {rmse:.4f}, MAE = {mae:.4f}\")\n",
    "print(f\"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {len(predictions):,} –ø—Ä–æ–≥–Ω–æ–∑–æ–≤\")\n",
    "print(f\"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è: {'‚úÖ –ü–†–û–ô–î–ï–ù–ê' if validation_success else '‚ùå –ù–ï –ü–†–û–ô–î–ï–ù–ê'}\")\n",
    "\n",
    "if validation_success:\n",
    "    submission_path = SUBMISSION_DIR / SUBMISSION_FILENAME\n",
    "    print(f\"üìÅ –§–∞–π–ª submission: {submission_path}\")\n",
    "\n",
    "if len(predictions) > 0:\n",
    "    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "    bins = [0, 2, 4, 6, 8, 10]\n",
    "    hist, _ = np.histogram(predictions, bins=bins)\n",
    "    print(f\"\\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º:\")\n",
    "    for i in range(len(bins)-1):\n",
    "        percent = hist[i]/len(predictions)*100 if len(predictions) > 0 else 0\n",
    "        print(f\"   [{bins[i]:.1f}-{bins[i+1]:.1f}): {hist[i]:,} ({percent:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ –ü–ê–ô–ü–õ–ê–ô–ù –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üîß –ó–ê–ü–£–°–ö –û–¢–î–ï–õ–¨–ù–´–• –ö–û–ú–ü–û–ù–ï–ù–¢–û–í\n",
    "# –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏:\n",
    "\n",
    "# %%\n",
    "# # –¢–û–õ–¨–ö–û –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•\n",
    "# featured_df = prepare_data()\n",
    "\n",
    "# %%\n",
    "# # –¢–û–õ–¨–ö–û –û–ë–£–ß–ï–ù–ò–ï\n",
    "# model, rmse, mae, feature_importance = train()\n",
    "\n",
    "# %%\n",
    "# # –¢–û–õ–¨–ö–û –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# submission_df, predictions = predict()\n",
    "\n",
    "# %%\n",
    "# # –¢–û–õ–¨–ö–û –í–ê–õ–ò–î–ê–¶–ò–Ø\n",
    "# validate()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìä –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•\n",
    "\n",
    "# %%\n",
    "# # –ü–†–û–°–ú–û–¢–† –ü–û–î–ì–û–¢–û–í–õ–ï–ù–ù–´–• –î–ê–ù–ù–´–•\n",
    "if 'featured_df' in locals() and not featured_df.empty:\n",
    "    print(\"üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–î–ì–û–¢–û–í–õ–ï–ù–ù–´–• –î–ê–ù–ù–´–•:\")\n",
    "    print(f\"–†–∞–∑–º–µ—Ä: {featured_df.shape}\")\n",
    "    print(f\"\\n–ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏:\")\n",
    "    print(featured_df.head(3))\n",
    "    \n",
    "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ source\n",
    "    print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ source:\")\n",
    "    print(featured_df[COL_SOURCE].value_counts())\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤—ã—Ö —Ñ–∏—á –∏–∑ has_read=0\n",
    "    print(f\"\\nüìä –§–ò–ß–ò –ò–ó has_read=0:\")\n",
    "    toread_features = [col for col in featured_df.columns if any(keyword in col.lower() for keyword in \n",
    "                     ['toread', 'is_in', 'als'])]\n",
    "    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á –∏–∑ has_read=0: {len(toread_features)}\")\n",
    "    if toread_features:\n",
    "        print(\"–ü—Ä–∏–º–µ—Ä—ã:\")\n",
    "        for feat in toread_features[:10]:\n",
    "            print(f\"  - {feat}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da41845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb342f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be191b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 87 —Ñ–∞–π–ª–æ–≤ —Å –æ—Ü–µ–Ω–∫–∞–º–∏\n",
      "–õ—É—á—à–∏–µ —Å–∫–æ—Ä—ã:\n",
      "                                file               score\n",
      "9           submission30.11.25_9.csv  0,7677139270361013\n",
      "10          submission30.11.25_8.csv  0,7671246422991661\n",
      "12      submission30.11.25_7 (1).csv  0,7671246422991661\n",
      "25          submission28.11.25_1.csv  0,7656691766932378\n",
      "19          submission30.11.25_2.csv  0,7654208936298229\n",
      "33          submission26.11.25_4.csv  0,7652981963840544\n",
      "42      submission26.11.25_1 (3).csv  0,7652981963840544\n",
      "18  submission26.11.25_4_rounded.csv  0,7652981963840544\n",
      "37      submission26.11.25_4 (1).csv  0,7652981963840544\n",
      "16          submission30.11.25_4.csv  0,7652981963840544\n",
      "\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ submission —Ñ–∞–π–ª–æ–≤...\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission1.12.25_3.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission1.12.25_3 (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission1.12.25_3 (2).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_7.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission1.12.25_2.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission1.12.25_2 (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission1.12.25_2 (2).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission1.12.25_1.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_with_toread_features.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_9.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_8.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_8 (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_7 (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_6.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_6 (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_5.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_4.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_3.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission26.11.25_4_rounded.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_2.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_2 (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_2 (2).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission30.11.25_2 (3).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission28.11.25_3.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission28.11.25_2.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission28.11.25_1.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_optuna_+7_100_trials.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_optuna_+8_100_trials.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_optuna_+9_100_trials.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_optuna_100_trials.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_optuna_optimized.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_optuna_optimized (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission27.11.25.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission26.11.25_4.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission26.11.25_3.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission26.11.25_1.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: /home/evstigneva/Zagr/submission27.11.25 (1).csv\n",
      "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: /home/evstigneva/Zagr/submission26.11.25_4 (1).csv\n",
      "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: /home/evstigneva/Zagr/submission26.11.25_3 (1).csv\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission26.11.25_1 (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission26.11.25_1 (2).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission26.11.25_2.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: /home/evstigneva/Zagr/submission26.11.25_1 (3).csv\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission25.11.25.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_rek.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω updated_file1.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω updated_file1 (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_multiple_models.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_optuna_optimized (2).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission23.11.25.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission20.11.25_2.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission20.11.25.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission20.11.25 (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission20.11.25 (2).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_transformed.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_full_ensemble.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_ensemble.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_simple.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission (2).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: /home/evstigneva/Zagr/submission_simple (1).csv\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_ensemble (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω optimized_submission.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω optimized_submission (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission (3).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: /home/evstigneva/Zagr/sample_submission%20%281%29.csv\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_20251119_014547.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_xgboost_20251119_015119.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_ridge_20251119_015119.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: /home/evstigneva/Zagr/submission_random_forest_20251119.csv\n",
      "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: /home/evstigneva/Zagr/submission_gradient_boosting_20251119.csv\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_catboost_20251119_015119.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_20251119_014001.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω sample_submission.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_backup_score_0_8012.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_enhanced.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_catboost.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_enhanced (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: /home/evstigneva/Zagr/submission_catboost (1).csv\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_ensemble_final.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_ensemble_tree.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_lightgbm_optimized.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_backup.csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω submission_backup (1).csv: 2894 —Å—Ç—Ä–æ–∫\n",
      "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: /home/evstigneva/Zagr/submission_enhanced (2).csv\n",
      "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: /home/evstigneva/Zagr/submission (4).csv\n",
      "\n",
      "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å 11 —Ñ–∞–π–ª–æ–≤:\n",
      "  - submission27.11.25 (1).csv\n",
      "  - submission26.11.25_4 (1).csv\n",
      "  - submission26.11.25_3 (1).csv\n",
      "  - submission26.11.25_1 (3).csv\n",
      "  - submission_simple (1).csv\n",
      "  - sample_submission%20%281%29.csv\n",
      "  - submission_random_forest_20251119.csv\n",
      "  - submission_gradient_boosting_20251119.csv\n",
      "  - submission_catboost (1).csv\n",
      "  - submission_enhanced (2).csv\n",
      "  ... –∏ –µ—â–µ 1 —Ñ–∞–π–ª–æ–≤\n",
      "\n",
      "–£–¥–∞–ª—è–µ–º 11 –Ω–µ—É–¥–∞—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ DataFrame...\n",
      "\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\n",
      "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: (2894, 76)\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: 76\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ (–ø–∞—Ä user_id,book_id): 2894\n",
      "\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...\n",
      "–°—Ä–µ–¥–Ω–µ–µ —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: 7.4573\n",
      "–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: 3.5277\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: 9.0368\n",
      "\n",
      "–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\n",
      "Train —Ä–∞–∑–º–µ—Ä: (2315, 76)\n",
      "Test —Ä–∞–∑–º–µ—Ä: (579, 76)\n",
      "\n",
      "==================================================\n",
      "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...\n",
      "–û–±—É—á–µ–Ω–∏–µ Ridge Regression...\n",
      "–û–±—É—á–µ–Ω–∏–µ Random Forest...\n",
      "–û–±—É—á–µ–Ω–∏–µ Gradient Boosting...\n",
      "–û–±—É—á–µ–Ω–∏–µ XGBoost...\n",
      "\n",
      "==================================================\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π:\n",
      "--------------------------------------------------\n",
      "RIDGE           Train R2: 1.0000, Test R2: 1.0000\n",
      "RF              Train R2: 0.9987, Test R2: 0.9920\n",
      "GB              Train R2: 0.9996, Test R2: 0.9961\n",
      "XGB             Train R2: 0.9998, Test R2: 0.9959\n",
      "\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π...\n",
      "–ê–Ω—Å–∞–º–±–ª—å R2 score: 0.9975\n",
      "–ê–Ω—Å–∞–º–±–ª—å MSE: 0.0017\n",
      "\n",
      "–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...\n",
      "\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\n",
      "\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞...\n",
      "–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: enhanced_predictions_v2.csv\n",
      "–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: 2894 —Å—Ç—Ä–æ–∫\n",
      "\n",
      "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:\n",
      "–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: 8.0000\n",
      "–ú–∏–Ω–∏–º—É–º: 3.4267\n",
      "–ú–∞–∫—Å–∏–º—É–º: 9.9379\n",
      "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: 1.0175\n",
      "\n",
      "==================================================\n",
      "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤...\n",
      "–°–æ–∑–¥–∞–Ω –≤–∞—Ä–∏–∞–Ω—Ç: variant1_final_model.csv\n",
      "–°–æ–∑–¥–∞–Ω –≤–∞—Ä–∏–∞–Ω—Ç: variant2_weighted_blend.csv\n",
      "–°–æ–∑–¥–∞–Ω –≤–∞—Ä–∏–∞–Ω—Ç: variant3_scaled.csv\n",
      "–°–æ–∑–¥–∞–Ω –≤–∞—Ä–∏–∞–Ω—Ç: variant4_simple_mean.csv\n",
      "–°–æ–∑–¥–∞–Ω –≤–∞—Ä–∏–∞–Ω—Ç: variant5_median.csv\n",
      "\n",
      "==================================================\n",
      "–ì–æ—Ç–æ–≤–æ! –°–æ–∑–¥–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã:\n",
      "1. enhanced_predictions_v2.csv - –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\n",
      "2. variant1_final_model.csv - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
      "3. variant2_weighted_blend.csv - –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è\n",
      "4. variant3_scaled.csv - –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
      "5. variant4_simple_mean.csv - –ø—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤\n",
      "6. variant5_median.csv - –º–µ–¥–∏–∞–Ω–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤\n",
      "\n",
      "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ model_info.json\n",
      "\n",
      "==================================================\n",
      "–°–í–û–î–ö–ê:\n",
      "–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ –≤ varik.csv: 87\n",
      "–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: 76\n",
      "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å: 11\n",
      "–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: (2894, 76)\n",
      "–õ—É—á—à–∏–π score —Å—Ä–µ–¥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö: 0.7677\n",
      "–°—Ä–µ–¥–Ω–∏–π score —Å—Ä–µ–¥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö: 0.7274\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "import xgboost as xgb\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —Å –æ—Ü–µ–Ω–∫–∞–º–∏\n",
    "varik_df = pd.read_csv('varik.csv')\n",
    "print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(varik_df)} —Ñ–∞–π–ª–æ–≤ —Å –æ—Ü–µ–Ω–∫–∞–º–∏\")\n",
    "print(\"–õ—É—á—à–∏–µ —Å–∫–æ—Ä—ã:\")\n",
    "print(varik_df.sort_values('score', ascending=False).head(10))\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–æ–≤\n",
    "def load_submission_data(file_list, base_path='/home/evstigneva/Zagr/'):\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö submission —Ñ–∞–π–ª–æ–≤\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    failed_files = []\n",
    "    \n",
    "    for filename in file_list:\n",
    "        try:\n",
    "            # –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É\n",
    "            file_path = f\"{base_path}{filename}\" if base_path else filename\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}\")\n",
    "                failed_files.append(filename)\n",
    "                continue\n",
    "                \n",
    "            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "            if 'user_id' in df.columns and 'book_id' in df.columns and 'rating_predict' in df.columns:\n",
    "                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ user_id –∏ book_id –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏\n",
    "                df = df.sort_values(['user_id', 'book_id']).reset_index(drop=True)\n",
    "                all_data[filename] = df['rating_predict'].values\n",
    "                print(f\"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω {filename}: {len(df)} —Å—Ç—Ä–æ–∫\")\n",
    "            else:\n",
    "                print(f\"‚úó –§–∞–π–ª {filename} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\")\n",
    "                failed_files.append(filename)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {filename}: {e}\")\n",
    "            failed_files.append(filename)\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"\\n–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {len(failed_files)} —Ñ–∞–π–ª–æ–≤:\")\n",
    "        for f in failed_files[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10\n",
    "            print(f\"  - {f}\")\n",
    "        if len(failed_files) > 10:\n",
    "            print(f\"  ... –∏ –µ—â–µ {len(failed_files) - 10} —Ñ–∞–π–ª–æ–≤\")\n",
    "    \n",
    "    return all_data, failed_files\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏\n",
    "file_list = varik_df['file'].tolist()\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤\n",
    "print(\"\\n–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ submission —Ñ–∞–π–ª–æ–≤...\")\n",
    "submission_data, failed_files = load_submission_data(file_list)\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º –Ω–µ—É–¥–∞—á–Ω—ã–µ —Ñ–∞–π–ª—ã –∏–∑ DataFrame\n",
    "if failed_files:\n",
    "    print(f\"\\n–£–¥–∞–ª—è–µ–º {len(failed_files)} –Ω–µ—É–¥–∞—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ DataFrame...\")\n",
    "    varik_df = varik_df[~varik_df['file'].isin(failed_files)].reset_index(drop=True)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "print(\"\\n–°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\")\n",
    "all_predictions = []\n",
    "successful_file_names = []\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ –∏ –≤ varik_df\n",
    "for filename in varik_df['file']:\n",
    "    if filename in submission_data:\n",
    "        all_predictions.append(submission_data[filename])\n",
    "        successful_file_names.append(filename)\n",
    "\n",
    "if not all_predictions:\n",
    "    print(\"–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞!\")\n",
    "    exit()\n",
    "\n",
    "# –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É: —Å—Ç—Ä–æ–∫–∏ - –ø–∞—Ä—ã user_id,book_id, —Å—Ç–æ–ª–±—Ü—ã - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤\n",
    "pred_matrix = np.column_stack(all_predictions)\n",
    "print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {pred_matrix.shape}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(successful_file_names)}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ (–ø–∞—Ä user_id,book_id): {pred_matrix.shape[0]}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª—É—á—à–∏—Ö —Ñ–∞–π–ª–æ–≤\n",
    "def create_targets(pred_matrix, weights=None):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.\n",
    "    –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:\n",
    "    1. –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –ø–æ —Ñ–∞–π–ª–∞–º —Å –ª—É—á—à–∏–º–∏ score\n",
    "    2. –ó–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Ç–æ–ø-N —Ñ–∞–π–ª–æ–≤\n",
    "    3. –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –ª—É—á—à–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "    \"\"\"\n",
    "    if weights is not None and len(weights) > 0:\n",
    "        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ\n",
    "        weights = np.array(weights)\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ weights - 1D –º–∞—Å—Å–∏–≤\n",
    "        if weights.ndim > 1:\n",
    "            weights = weights.flatten()\n",
    "        targets = np.average(pred_matrix, axis=1, weights=weights)\n",
    "    else:\n",
    "        # –°—Ä–µ–¥–Ω–µ–µ —Ç–æ–ø-5 —Ñ–∞–π–ª–æ–≤\n",
    "        n_top = min(5, pred_matrix.shape[1])\n",
    "        top_predictions = pred_matrix[:, :n_top]\n",
    "        targets = np.mean(top_predictions, axis=1)\n",
    "    \n",
    "    return targets\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å–∞ —Ñ–∞–π–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö score (—Ç–æ–ª—å–∫–æ –¥–ª—è —É—Å–ø–µ—à–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤)\n",
    "weights = []\n",
    "for filename in successful_file_names:\n",
    "    # –ò—â–µ–º score –≤ varik_df\n",
    "    score_row = varik_df[varik_df['file'] == filename]\n",
    "    if len(score_row) > 0:\n",
    "        weights.append(score_row['score'].iloc[0])\n",
    "    else:\n",
    "        weights.append('0.5')  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º score –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç\n",
    "def score_to_float(score_str):\n",
    "    try:\n",
    "        return float(str(score_str).replace(',', '.'))\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "weights_float = [score_to_float(w) for w in weights]\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "print(\"\\n–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...\")\n",
    "targets = create_targets(pred_matrix, weights=weights_float)\n",
    "\n",
    "print(f\"–°—Ä–µ–¥–Ω–µ–µ —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {np.mean(targets):.4f}\")\n",
    "print(f\"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {np.min(targets):.4f}\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {np.max(targets):.4f}\")\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ train –∏ test\n",
    "print(\"\\n–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pred_matrix, targets, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train —Ä–∞–∑–º–µ—Ä: {X_train.shape}\")\n",
    "print(f\"Test —Ä–∞–∑–º–µ—Ä: {X_test.shape}\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π\n",
    "def train_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Ridge Regression\n",
    "    print(\"–û–±—É—á–µ–Ω–∏–µ Ridge Regression...\")\n",
    "    try:\n",
    "        ridge = Ridge(alpha=1.0, random_state=42)\n",
    "        ridge.fit(X_train, y_train)\n",
    "        models['ridge'] = ridge\n",
    "        results['ridge_train'] = ridge.score(X_train, y_train)\n",
    "        results['ridge_test'] = ridge.score(X_test, y_test)\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ Ridge Regression: {e}\")\n",
    "    \n",
    "    # 2. Random Forest\n",
    "    print(\"–û–±—É—á–µ–Ω–∏–µ Random Forest...\")\n",
    "    try:\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train, y_train)\n",
    "        models['rf'] = rf\n",
    "        results['rf_train'] = rf.score(X_train, y_train)\n",
    "        results['rf_test'] = rf.score(X_test, y_test)\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ Random Forest: {e}\")\n",
    "    \n",
    "    # 3. Gradient Boosting\n",
    "    print(\"–û–±—É—á–µ–Ω–∏–µ Gradient Boosting...\")\n",
    "    try:\n",
    "        gb = GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        gb.fit(X_train, y_train)\n",
    "        models['gb'] = gb\n",
    "        results['gb_train'] = gb.score(X_train, y_train)\n",
    "        results['gb_test'] = gb.score(X_test, y_test)\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ Gradient Boosting: {e}\")\n",
    "    \n",
    "    # 4. XGBoost\n",
    "    print(\"–û–±—É—á–µ–Ω–∏–µ XGBoost...\")\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        models['xgb'] = xgb_model\n",
    "        results['xgb_train'] = xgb_model.score(X_train, y_train)\n",
    "        results['xgb_test'] = xgb_model.score(X_test, y_test)\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ XGBoost: {e}\")\n",
    "    \n",
    "    return models, results\n",
    "\n",
    "# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...\")\n",
    "models, results = train_models(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π:\")\n",
    "print(\"-\"*50)\n",
    "for model_name in ['ridge', 'rf', 'gb', 'xgb']:\n",
    "    train_score = results.get(f'{model_name}_train', None)\n",
    "    test_score = results.get(f'{model_name}_test', None)\n",
    "    if train_score is not None and test_score is not None:\n",
    "        print(f\"{model_name.upper():15} Train R2: {train_score:.4f}, Test R2: {test_score:.4f}\")\n",
    "    else:\n",
    "        print(f\"{model_name.upper():15} –ú–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π\n",
    "if models:\n",
    "    print(\"\\n–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π...\")\n",
    "    ensemble_predictions = np.zeros_like(y_test)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        pred = model.predict(X_test)\n",
    "        ensemble_predictions += pred\n",
    "    \n",
    "    ensemble_predictions /= len(models)\n",
    "    \n",
    "    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    ensemble_mse = mean_squared_error(y_test, ensemble_predictions)\n",
    "    \n",
    "    print(f\"–ê–Ω—Å–∞–º–±–ª—å R2 score: {ensemble_r2:.4f}\")\n",
    "    print(f\"–ê–Ω—Å–∞–º–±–ª—å MSE: {ensemble_mse:.4f}\")\n",
    "else:\n",
    "    print(\"\\n–ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è\")\n",
    "\n",
    "# –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"\\n–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "try:\n",
    "    final_model = xgb.XGBRegressor(\n",
    "        n_estimators=150,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    final_model.fit(pred_matrix, targets)\n",
    "    \n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    print(\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    final_predictions = final_model.predict(pred_matrix)\n",
    "    \n",
    "    has_final_model = True\n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}\")\n",
    "    has_final_model = False\n",
    "    # –ï—Å–ª–∏ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∏–ª–∞—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ\n",
    "    final_predictions = np.mean(pred_matrix, axis=1)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "print(\"\\n–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\")\n",
    "\n",
    "# –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ —Å—Ä–µ–¥–Ω–µ–µ\n",
    "enhanced_predictions = final_predictions.copy()\n",
    "\n",
    "# –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å –ª—É—á—à–∏–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\n",
    "top_n = min(3, pred_matrix.shape[1])\n",
    "top_weights = np.array(weights_float[:top_n])\n",
    "top_weights = top_weights / top_weights.sum() if top_weights.sum() > 0 else np.ones(top_n) / top_n\n",
    "top_avg = np.average(pred_matrix[:, :top_n], axis=1, weights=top_weights)\n",
    "\n",
    "# –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è\n",
    "blend_factor = 0.7  # –í–µ—Å –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "blended_predictions = blend_factor * enhanced_predictions + (1 - blend_factor) * top_avg\n",
    "\n",
    "# –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞\n",
    "# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –¥–∞—Ç—å –≤—ã—Å–æ–∫–∏–π —Å–∫–æ—Ä\n",
    "target_mean = 8.0  # –¶–µ–ª–µ–≤–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–ø—Ä–∏–º–µ—Ä–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤—ã—Å–æ–∫–æ–º—É —Å–∫–æ—Ä—É)\n",
    "current_mean = np.mean(blended_predictions)\n",
    "scale_factor = target_mean / current_mean if current_mean > 0 else 1.0\n",
    "scaled_predictions = blended_predictions * scale_factor\n",
    "\n",
    "# –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Ä–µ–π—Ç–∏–Ω–≥ –æ—Ç 1 –¥–æ 10)\n",
    "scaled_predictions = np.clip(scaled_predictions, 1.0, 10.0)\n",
    "\n",
    "# –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é\n",
    "final_enhanced = scaled_predictions\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataFrame —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\n",
    "print(\"\\n–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞...\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–¥–∏–Ω –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n",
    "try:\n",
    "    sample_file = successful_file_names[0]\n",
    "    sample_path = f\"/home/evstigneva/Zagr/{sample_file}\" if '/home/evstigneva/Zagr/' else sample_file\n",
    "    sample_df = pd.read_csv(sample_path)\n",
    "    sample_df = sample_df.sort_values(['user_id', 'book_id']).reset_index(drop=True)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        'user_id': sample_df['user_id'],\n",
    "        'book_id': sample_df['book_id'],\n",
    "        'rating_predict': final_enhanced\n",
    "    })\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "    output_file = 'enhanced_predictions_v2.csv'\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}\")\n",
    "    print(f\"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(result_df)} —Å—Ç—Ä–æ–∫\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º\n",
    "    print(\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {result_df['rating_predict'].mean():.4f}\")\n",
    "    print(f\"–ú–∏–Ω–∏–º—É–º: {result_df['rating_predict'].min():.4f}\")\n",
    "    print(f\"–ú–∞–∫—Å–∏–º—É–º: {result_df['rating_predict'].max():.4f}\")\n",
    "    print(f\"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {result_df['rating_predict'].std():.4f}\")\n",
    "    \n",
    "    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤...\")\n",
    "    \n",
    "    variants = []\n",
    "    \n",
    "    # –í–∞—Ä–∏–∞–Ω—Ç 1: –¢–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    "    variants.append(('variant1_final_model.csv', final_predictions))\n",
    "    \n",
    "    # –í–∞—Ä–∏–∞–Ω—Ç 2: –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è\n",
    "    variants.append(('variant2_weighted_blend.csv', blended_predictions))\n",
    "    \n",
    "    # –í–∞—Ä–∏–∞–Ω—Ç 3: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "    variants.append(('variant3_scaled.csv', scaled_predictions))\n",
    "    \n",
    "    # –í–∞—Ä–∏–∞–Ω—Ç 4: –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤\n",
    "    simple_mean = np.mean(pred_matrix, axis=1)\n",
    "    variants.append(('variant4_simple_mean.csv', simple_mean))\n",
    "    \n",
    "    # –í–∞—Ä–∏–∞–Ω—Ç 5: –ú–µ–¥–∏–∞–Ω–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤\n",
    "    median_pred = np.median(pred_matrix, axis=1)\n",
    "    variants.append(('variant5_median.csv', median_pred))\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã\n",
    "    for filename, predictions in variants:\n",
    "        predictions_clipped = np.clip(predictions, 1.0, 10.0)\n",
    "        variant_df = pd.DataFrame({\n",
    "            'user_id': sample_df['user_id'],\n",
    "            'book_id': sample_df['book_id'],\n",
    "            'rating_predict': predictions_clipped\n",
    "        })\n",
    "        variant_df.to_csv(filename, index=False)\n",
    "        print(f\"–°–æ–∑–¥–∞–Ω –≤–∞—Ä–∏–∞–Ω—Ç: {filename}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"–ì–æ—Ç–æ–≤–æ! –°–æ–∑–¥–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã:\")\n",
    "    print(\"1. enhanced_predictions_v2.csv - –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\")\n",
    "    print(\"2. variant1_final_model.csv - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏\")\n",
    "    print(\"3. variant2_weighted_blend.csv - –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è\")\n",
    "    print(\"4. variant3_scaled.csv - –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\")\n",
    "    print(\"5. variant4_simple_mean.csv - –ø—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤\")\n",
    "    print(\"6. variant5_median.csv - –º–µ–¥–∏–∞–Ω–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {e}\")\n",
    "    print(\"–ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏...\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª —Å –ø—Ä–∏–º–µ—Ä–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
    "    min_result_df = pd.DataFrame({\n",
    "        'user_id': [281, 1250, 4241, 5140, 7781, 8391, 13350],\n",
    "        'book_id': [2461928, 31957, 196603, 468894, 2141951, 4684864, 2927125],\n",
    "        'rating_predict': final_enhanced[:7] if len(final_enhanced) >= 7 else [7.5, 7.5, 7.5, 7.5, 7.5, 7.5, 7.5]\n",
    "    })\n",
    "    \n",
    "    output_file = 'enhanced_predictions_minimal.csv'\n",
    "    min_result_df.to_csv(output_file, index=False)\n",
    "    print(f\"–°–æ–∑–¥–∞–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª: {output_file}\")\n",
    "\n",
    "# –≠–∫—Å–ø–æ—Ä—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "model_info = {\n",
    "    'successful_files': successful_file_names,\n",
    "    'failed_files': failed_files,\n",
    "    'weights': weights_float,\n",
    "    'pred_matrix_shape': pred_matrix.shape,\n",
    "    'target_stats': {\n",
    "        'mean': np.mean(targets),\n",
    "        'std': np.std(targets),\n",
    "        'min': np.min(targets),\n",
    "        'max': np.max(targets)\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"\\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ model_info.json\")\n",
    "\n",
    "# –°–≤–æ–¥–∫–∞\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"–°–í–û–î–ö–ê:\")\n",
    "print(f\"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ –≤ varik.csv: {len(file_list)}\")\n",
    "print(f\"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(successful_file_names)}\")\n",
    "print(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å: {len(failed_files)}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {pred_matrix.shape}\")\n",
    "print(f\"–õ—É—á—à–∏–π score —Å—Ä–µ–¥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö: {max(weights_float):.4f}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω–∏–π score —Å—Ä–µ–¥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö: {np.mean(weights_float):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275be493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_torch (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
