{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a13b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nto25_notebook.ipynb\n",
    "\n",
    "# %% [markdown]\n",
    "# # NTO25 ML Competition Baseline\n",
    "# –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é\n",
    "\n",
    "# %%\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)\n",
    "# !pip install lightgbm pandas numpy scikit-learn tqdm joblib pyarrow\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –ö–û–ù–°–¢–ê–ù–¢–´\n",
    "# =============================================================================\n",
    "\n",
    "# --- –§–ê–ô–õ–´ ---\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞\n",
    "TRAIN_FILENAME = \"/home/evstigneva/nto252/mix/train.csv\"\n",
    "TEST_FILENAME = \"/home/evstigneva/nto252/test.csv\" \n",
    "USER_DATA_FILENAME = \"/home/evstigneva/nto252/mix/users.csv\"\n",
    "BOOK_DATA_FILENAME = \"/home/evstigneva/nto252/mix/books.csv\"\n",
    "BOOK_GENRES_FILENAME = \"/home/evstigneva/nto252/mix/book_genres.csv\"\n",
    "GENRES_FILENAME = \"/home/evstigneva/nto252/mix/genres.csv\"\n",
    "BOOK_DESCRIPTIONS_FILENAME = \"/home/evstigneva/nto252/mix/book_descriptions.csv\"\n",
    "SUBMISSION_FILENAME = \"/home/evstigneva/nto252/submission30.11.25_7.csv\"\n",
    "TFIDF_VECTORIZER_FILENAME = \"tfidf_vectorizer.pkl\"\n",
    "PROCESSED_DATA_FILENAME = \"processed_features.parquet\"\n",
    "\n",
    "# --- –ù–ê–ó–í–ê–ù–ò–Ø –ö–û–õ–û–ù–û–ö ---\n",
    "COL_USER_ID = \"user_id\"\n",
    "COL_BOOK_ID = \"book_id\"\n",
    "COL_TARGET = \"rating\"\n",
    "COL_SOURCE = \"source\"\n",
    "COL_PREDICTION = \"rating_predict\"\n",
    "COL_HAS_READ = \"has_read\"\n",
    "COL_TIMESTAMP = \"timestamp\"\n",
    "\n",
    "# –§–∏—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–Ω–æ–≤—ã–µ)\n",
    "F_USER_MEAN_RATING = \"user_mean_rating\"\n",
    "F_USER_RATINGS_COUNT = \"user_ratings_count\"\n",
    "F_BOOK_MEAN_RATING = \"book_mean_rating\"\n",
    "F_BOOK_RATINGS_COUNT = \"book_ratings_count\"\n",
    "F_AUTHOR_MEAN_RATING = \"author_mean_rating\"\n",
    "F_BOOK_GENRES_COUNT = \"book_genres_count\"\n",
    "\n",
    "# –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "COL_GENDER = \"gender\"\n",
    "COL_AGE = \"age\"\n",
    "COL_AUTHOR_ID = \"author_id\"\n",
    "COL_PUBLICATION_YEAR = \"publication_year\"\n",
    "COL_LANGUAGE = \"language\"\n",
    "COL_PUBLISHER = \"publisher\"\n",
    "COL_AVG_RATING = \"avg_rating\"\n",
    "COL_GENRE_ID = \"genre_id\"\n",
    "COL_DESCRIPTION = \"description\"\n",
    "\n",
    "# --- –ó–ù–ê–ß–ï–ù–ò–Ø ---\n",
    "VAL_SOURCE_TRAIN = \"train\"\n",
    "VAL_SOURCE_TEST = \"test\"\n",
    "\n",
    "# --- –ú–ê–ì–ò–ß–ï–°–ö–ò–ï –ß–ò–°–õ–ê ---\n",
    "MISSING_CAT_VALUE = \"-1\"\n",
    "MISSING_NUM_VALUE = -1\n",
    "PREDICTION_MIN_VALUE = 0\n",
    "PREDICTION_MAX_VALUE = 10\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø\n",
    "# =============================================================================\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é\n",
    "ROOT_DIR = Path(\"/home/evstigneva/nto25/baseline\").resolve()\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "RAW_DATA_DIR = Path(\"/home/evstigneva/Zagr\")  # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
    "INTERIM_DATA_DIR = DATA_DIR / \"interim\" \n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "OUTPUT_DIR = ROOT_DIR / \"output\"\n",
    "MODEL_DIR = OUTPUT_DIR / \"models\"\n",
    "SUBMISSION_DIR = OUTPUT_DIR / \"submissions\"\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "for dir_path in [DATA_DIR, PROCESSED_DATA_DIR, MODEL_DIR, SUBMISSION_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ–∑–¥–∞–Ω—ã:\")\n",
    "print(f\"   - –î–∞–Ω–Ω—ã–µ: {DATA_DIR}\")\n",
    "print(f\"   - –ú–æ–¥–µ–ª–∏: {MODEL_DIR}\")\n",
    "print(f\"   - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {SUBMISSION_DIR}\")\n",
    "print(f\"   - –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {RAW_DATA_DIR}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"\\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "data_files = [\n",
    "    TRAIN_FILENAME, TEST_FILENAME, USER_DATA_FILENAME, \n",
    "    BOOK_DATA_FILENAME, BOOK_GENRES_FILENAME, GENRES_FILENAME, BOOK_DESCRIPTIONS_FILENAME\n",
    "]\n",
    "\n",
    "for file_path in data_files:\n",
    "    if Path(file_path).exists():\n",
    "        print(f\"   ‚úÖ {Path(file_path).name} - –Ω–∞–π–¥–µ–Ω\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {Path(file_path).name} - –ù–ï –ù–ê–ô–î–ï–ù\")\n",
    "\n",
    "# --- –ü–ê–†–ê–ú–ï–¢–†–´ ---\n",
    "RANDOM_STATE = 42\n",
    "TARGET = COL_TARGET\n",
    "\n",
    "# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –í–†–ï–ú–ï–ù–ù–û–ì–û –†–ê–ó–î–ï–õ–ï–ù–ò–Ø ---\n",
    "TEMPORAL_SPLIT_RATIO = 0.8\n",
    "\n",
    "# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø ---\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "MODEL_FILENAME = \"lgb_model.txt\"\n",
    "\n",
    "# --- –ü–ê–†–ê–ú–ï–¢–†–´ TF-IDF ---\n",
    "TFIDF_MAX_FEATURES = 500\n",
    "TFIDF_MIN_DF = 2\n",
    "TFIDF_MAX_DF = 0.95\n",
    "TFIDF_NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# --- –§–ò–ß–ò ---\n",
    "CAT_FEATURES = [\n",
    "    COL_USER_ID,\n",
    "    COL_BOOK_ID,\n",
    "    COL_GENDER,\n",
    "    COL_AGE,\n",
    "    COL_AUTHOR_ID,\n",
    "    COL_PUBLICATION_YEAR,\n",
    "    COL_LANGUAGE,\n",
    "    COL_PUBLISHER,\n",
    "]\n",
    "\n",
    "# --- –ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ò ---\n",
    "LGB_PARAMS = {\n",
    "    \"objective\": \"rmse\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"n_estimators\": 2000,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"lambda_l1\": 0.1,\n",
    "    \"lambda_l2\": 0.1,\n",
    "    \"num_leaves\": 31,\n",
    "    \"verbose\": -1,\n",
    "    \"n_jobs\": -1,\n",
    "    \"seed\": RANDOM_STATE,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "}\n",
    "\n",
    "LGB_FIT_PARAMS = {\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"callbacks\": [],\n",
    "}\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –§–£–ù–ö–¶–ò–ò –û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–•\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_merge_data():\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö –≤ –µ–¥–∏–Ω—ã–π DataFrame.\"\"\"\n",
    "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "    dtype_spec = {\n",
    "        COL_USER_ID: \"int32\",\n",
    "        COL_BOOK_ID: \"int32\",\n",
    "        COL_TARGET: \"float32\",\n",
    "        COL_GENDER: \"category\",\n",
    "        COL_AGE: \"float32\",\n",
    "        COL_AUTHOR_ID: \"int32\",\n",
    "        COL_PUBLICATION_YEAR: \"float32\",\n",
    "        COL_LANGUAGE: \"category\",\n",
    "        COL_PUBLISHER: \"category\",\n",
    "        COL_AVG_RATING: \"float32\",\n",
    "        COL_GENRE_ID: \"int16\",\n",
    "    }\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
    "    train_df = pd.read_csv(\n",
    "        TRAIN_FILENAME,\n",
    "        dtype={\n",
    "            k: v\n",
    "            for k, v in dtype_spec.items()\n",
    "            if k in [COL_USER_ID, COL_BOOK_ID, COL_TARGET]\n",
    "        },\n",
    "        parse_dates=[COL_TIMESTAMP],\n",
    "    )\n",
    "\n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: —Ç–æ–ª—å–∫–æ –∫–Ω–∏–≥–∏ —Å —Ä–µ–π—Ç–∏–Ω–≥–æ–º (has_read=1)\n",
    "    initial_count = len(train_df)\n",
    "    train_df = train_df[train_df[COL_HAS_READ] == 1].copy()\n",
    "    filtered_count = len(train_df)\n",
    "    print(f\"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {initial_count} -> {filtered_count} —Å—Ç—Ä–æ–∫ (—Ç–æ–ª—å–∫–æ has_read=1)\")\n",
    "    \n",
    "    test_df = pd.read_csv(\n",
    "        TEST_FILENAME,\n",
    "        dtype={k: v for k, v in dtype_spec.items() if k in [COL_USER_ID, COL_BOOK_ID]},\n",
    "    )\n",
    "    user_data_df = pd.read_csv(\n",
    "        USER_DATA_FILENAME,\n",
    "        dtype={\n",
    "            k: v for k, v in dtype_spec.items() if k in [COL_USER_ID, COL_GENDER, COL_AGE]\n",
    "        },\n",
    "    )\n",
    "    book_data_df = pd.read_csv(\n",
    "        BOOK_DATA_FILENAME,\n",
    "        dtype={\n",
    "            k: v\n",
    "            for k, v in dtype_spec.items()\n",
    "            if k\n",
    "            in [\n",
    "                COL_BOOK_ID,\n",
    "                COL_AUTHOR_ID,\n",
    "                COL_PUBLICATION_YEAR,\n",
    "                COL_LANGUAGE,\n",
    "                COL_AVG_RATING,\n",
    "                COL_PUBLISHER,\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "    book_genres_df = pd.read_csv(\n",
    "        BOOK_GENRES_FILENAME,\n",
    "        dtype={k: v for k, v in dtype_spec.items() if k in [COL_BOOK_ID, COL_GENRE_ID]},\n",
    "    )\n",
    "    genres_df = pd.read_csv(GENRES_FILENAME)\n",
    "    book_descriptions_df = pd.read_csv(\n",
    "        BOOK_DESCRIPTIONS_FILENAME,\n",
    "        dtype={COL_BOOK_ID: \"int32\"},\n",
    "    )\n",
    "\n",
    "    print(\"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\")\n",
    "\n",
    "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º train –∏ test\n",
    "    train_df[COL_SOURCE] = VAL_SOURCE_TRAIN\n",
    "    test_df[COL_SOURCE] = VAL_SOURCE_TEST\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)\n",
    "\n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
    "    combined_df = combined_df.merge(user_data_df, on=COL_USER_ID, how=\"left\")\n",
    "\n",
    "    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ book_data_df –ø–µ—Ä–µ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º\n",
    "    book_data_df = book_data_df.drop_duplicates(subset=[COL_BOOK_ID])\n",
    "    combined_df = combined_df.merge(book_data_df, on=COL_BOOK_ID, how=\"left\")\n",
    "\n",
    "    print(f\"–†–∞–∑–º–µ—Ä –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {combined_df.shape}\")\n",
    "    return combined_df, book_genres_df, genres_df, book_descriptions_df\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –§–£–ù–ö–¶–ò–ò –§–ò–ß–ï–ô\n",
    "# =============================================================================\n",
    "\n",
    "def add_aggregate_features(df, train_df):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –∫–Ω–∏–≥ –∏ –∞–≤—Ç–æ—Ä–æ–≤.\"\"\"\n",
    "    print(\"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á...\")\n",
    "\n",
    "    # –ê–≥—Ä–µ–≥–∞—Ç—ã –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º\n",
    "    user_agg = train_df.groupby(COL_USER_ID)[TARGET].agg([\"mean\", \"count\"]).reset_index()\n",
    "    user_agg.columns = [\n",
    "        COL_USER_ID,\n",
    "        F_USER_MEAN_RATING,\n",
    "        F_USER_RATINGS_COUNT,\n",
    "    ]\n",
    "\n",
    "    # –ê–≥—Ä–µ–≥–∞—Ç—ã –ø–æ –∫–Ω–∏–≥–∞–º\n",
    "    book_agg = train_df.groupby(COL_BOOK_ID)[TARGET].agg([\"mean\", \"count\"]).reset_index()\n",
    "    book_agg.columns = [\n",
    "        COL_BOOK_ID,\n",
    "        F_BOOK_MEAN_RATING,\n",
    "        F_BOOK_RATINGS_COUNT,\n",
    "    ]\n",
    "\n",
    "    # –ê–≥—Ä–µ–≥–∞—Ç—ã –ø–æ –∞–≤—Ç–æ—Ä–∞–º\n",
    "    author_agg = train_df.groupby(COL_AUTHOR_ID)[TARGET].agg([\"mean\"]).reset_index()\n",
    "    author_agg.columns = [COL_AUTHOR_ID, F_AUTHOR_MEAN_RATING]\n",
    "\n",
    "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∞–≥—Ä–µ–≥–∞—Ç—ã —Å –æ—Å–Ω–æ–≤–Ω—ã–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–º\n",
    "    df = df.merge(user_agg, on=COL_USER_ID, how=\"left\")\n",
    "    df = df.merge(book_agg, on=COL_BOOK_ID, how=\"left\")\n",
    "    return df.merge(author_agg, on=COL_AUTHOR_ID, how=\"left\")\n",
    "\n",
    "def add_genre_features(df, book_genres_df):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∂–∞–Ω—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–Ω–∏–≥–∏.\"\"\"\n",
    "    print(\"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏—á –∂–∞–Ω—Ä–æ–≤...\")\n",
    "    genre_counts = book_genres_df.groupby(COL_BOOK_ID)[COL_GENRE_ID].count().reset_index()\n",
    "    genre_counts.columns = [\n",
    "        COL_BOOK_ID,\n",
    "        F_BOOK_GENRES_COUNT,\n",
    "    ]\n",
    "    return df.merge(genre_counts, on=COL_BOOK_ID, how=\"left\")\n",
    "\n",
    "def add_text_features(df, train_df, descriptions_df):\n",
    "    \"\"\"–î–æ–±–∞–≤–ª—è–µ—Ç TF-IDF —Ñ–∏—á–∏ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏–π –∫–Ω–∏–≥.\"\"\"\n",
    "    print(\"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∏—á (TF-IDF)...\")\n",
    "\n",
    "    vectorizer_path = MODEL_DIR / TFIDF_VECTORIZER_FILENAME\n",
    "\n",
    "    # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–Ω–∏–≥–∏ –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n",
    "    train_books = train_df[COL_BOOK_ID].unique()\n",
    "\n",
    "    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∫–Ω–∏–≥\n",
    "    train_descriptions = descriptions_df[descriptions_df[COL_BOOK_ID].isin(train_books)].copy()\n",
    "    train_descriptions[COL_DESCRIPTION] = train_descriptions[COL_DESCRIPTION].fillna(\"\")\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä–∞ (–¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)\n",
    "    if vectorizer_path.exists():\n",
    "        print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä–∞ –∏–∑ {vectorizer_path}\")\n",
    "        vectorizer = joblib.load(vectorizer_path)\n",
    "    else:\n",
    "        # –û–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏—è—Ö\n",
    "        print(\"–û–±—É—á–µ–Ω–∏–µ TF-IDF –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä–∞ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏—è—Ö...\")\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=TFIDF_MAX_FEATURES,\n",
    "            min_df=TFIDF_MIN_DF,\n",
    "            max_df=TFIDF_MAX_DF,\n",
    "            ngram_range=TFIDF_NGRAM_RANGE,\n",
    "        )\n",
    "        vectorizer.fit(train_descriptions[COL_DESCRIPTION])\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏\n",
    "        joblib.dump(vectorizer, vectorizer_path)\n",
    "        print(f\"–í–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {vectorizer_path}\")\n",
    "\n",
    "    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –≤—Å–µ –æ–ø–∏—Å–∞–Ω–∏—è –∫–Ω–∏–≥\n",
    "    all_descriptions = descriptions_df[[COL_BOOK_ID, COL_DESCRIPTION]].copy()\n",
    "    all_descriptions[COL_DESCRIPTION] = all_descriptions[COL_DESCRIPTION].fillna(\"\")\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ book_id -> description\n",
    "    description_map = dict(\n",
    "        zip(all_descriptions[COL_BOOK_ID], all_descriptions[COL_DESCRIPTION], strict=False)\n",
    "    )\n",
    "\n",
    "    # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –∫–Ω–∏–≥ –≤ df (–≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ)\n",
    "    df_descriptions = df[COL_BOOK_ID].map(description_map).fillna(\"\")\n",
    "\n",
    "    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –≤ TF-IDF —Ñ–∏—á–∏\n",
    "    tfidf_matrix = vectorizer.transform(df_descriptions)\n",
    "\n",
    "    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –≤ DataFrame\n",
    "    tfidf_feature_names = [f\"tfidf_{i}\" for i in range(tfidf_matrix.shape[1])]\n",
    "    tfidf_df = pd.DataFrame(\n",
    "        tfidf_matrix.toarray(),\n",
    "        columns=tfidf_feature_names,\n",
    "        index=df.index,\n",
    "    )\n",
    "\n",
    "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º TF-IDF —Ñ–∏—á–∏ —Å –æ—Å–Ω–æ–≤–Ω—ã–º DataFrame\n",
    "    df_with_tfidf = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    print(f\"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(tfidf_feature_names)} TF-IDF —Ñ–∏—á.\")\n",
    "    return df_with_tfidf\n",
    "\n",
    "def handle_missing_values(df, train_df):\n",
    "    \"\"\"–ó–∞–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.\"\"\"\n",
    "    print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...\")\n",
    "\n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "    global_mean = train_df[TARGET].mean()\n",
    "\n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç –º–µ–¥–∏–∞–Ω–æ–π\n",
    "    age_median = df[COL_AGE].median()\n",
    "    df[COL_AGE] = df[COL_AGE].fillna(age_median)\n",
    "\n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏ –¥–ª—è \"—Ö–æ–ª–æ–¥–Ω–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞\" –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π/–ø—Ä–µ–¥–º–µ—Ç–æ–≤\n",
    "    if F_USER_MEAN_RATING in df.columns:\n",
    "        df[F_USER_MEAN_RATING] = df[F_USER_MEAN_RATING].fillna(global_mean)\n",
    "    if F_BOOK_MEAN_RATING in df.columns:\n",
    "        df[F_BOOK_MEAN_RATING] = df[F_BOOK_MEAN_RATING].fillna(global_mean)\n",
    "    if F_AUTHOR_MEAN_RATING in df.columns:\n",
    "        df[F_AUTHOR_MEAN_RATING] = df[F_AUTHOR_MEAN_RATING].fillna(global_mean)\n",
    "\n",
    "    if F_USER_RATINGS_COUNT in df.columns:\n",
    "        df[F_USER_RATINGS_COUNT] = df[F_USER_RATINGS_COUNT].fillna(0)\n",
    "    if F_BOOK_RATINGS_COUNT in df.columns:\n",
    "        df[F_BOOK_RATINGS_COUNT] = df[F_BOOK_RATINGS_COUNT].fillna(0)\n",
    "\n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º avg_rating –∏–∑ book_data –≥–ª–æ–±–∞–ª—å–Ω—ã–º —Å—Ä–µ–¥–Ω–∏–º\n",
    "    df[COL_AVG_RATING] = df[COL_AVG_RATING].fillna(global_mean)\n",
    "\n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –∂–∞–Ω—Ä–æ–≤ –Ω—É–ª—è–º–∏\n",
    "    df[F_BOOK_GENRES_COUNT] = df[F_BOOK_GENRES_COUNT].fillna(0)\n",
    "\n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º TF-IDF —Ñ–∏—á–∏ –Ω—É–ª—è–º–∏ (–¥–ª—è –∫–Ω–∏–≥ –±–µ–∑ –æ–ø–∏—Å–∞–Ω–∏–π)\n",
    "    tfidf_cols = [col for col in df.columns if col.startswith(\"tfidf_\")]\n",
    "    for col in tfidf_cols:\n",
    "        df[col] = df[col].fillna(0.0)\n",
    "\n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
    "    for col in CAT_FEATURES:\n",
    "        if col in df.columns:\n",
    "            if df[col].dtype.name in (\"category\", \"object\") and df[col].isna().any():\n",
    "                df[col] = df[col].astype(str).fillna(MISSING_CAT_VALUE).astype(\"category\")\n",
    "            elif pd.api.types.is_numeric_dtype(df[col].dtype) and df[col].isna().any():\n",
    "                df[col] = df[col].fillna(MISSING_NUM_VALUE)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_features(df, book_genres_df, descriptions_df, include_aggregates=False):\n",
    "    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ —Ñ–∏—á.\"\"\"\n",
    "    print(\"–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ —Ñ–∏—á...\")\n",
    "    train_df = df[df[COL_SOURCE] == VAL_SOURCE_TRAIN].copy()\n",
    "\n",
    "    # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è\n",
    "    if include_aggregates:\n",
    "        df = add_aggregate_features(df, train_df)\n",
    "\n",
    "    df = add_genre_features(df, book_genres_df)\n",
    "    df = add_text_features(df, train_df, descriptions_df)\n",
    "    df = handle_missing_values(df, train_df)\n",
    "\n",
    "    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ pandas 'category' dtype –¥–ª—è LightGBM\n",
    "    for col in CAT_FEATURES:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    print(\"–ò–Ω–∂–µ–Ω–µ—Ä–∏—è —Ñ–∏—á –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")\n",
    "    return df\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –§–£–ù–ö–¶–ò–ò –í–†–ï–ú–ï–ù–ù–û–ì–û –†–ê–ó–î–ï–õ–ï–ù–ò–Ø\n",
    "# =============================================================================\n",
    "\n",
    "def temporal_split_by_date(df, split_date, timestamp_col=COL_TIMESTAMP):\n",
    "    \"\"\"–†–∞–∑–¥–µ–ª—è–µ—Ç DataFrame –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏ –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π –¥–∞—Ç–µ.\"\"\"\n",
    "    if timestamp_col not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"–ö–æ–ª–æ–Ω–∫–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ '{timestamp_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ DataFrame. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –≤ datetime —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):\n",
    "        df = df.copy()\n",
    "        df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "\n",
    "    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –ø–æ—Ä–æ–≥—É –¥–∞—Ç—ã\n",
    "    train_mask = df[timestamp_col] <= split_date\n",
    "    val_mask = df[timestamp_col] > split_date\n",
    "\n",
    "    # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "    if train_mask.sum() == 0:\n",
    "        raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π <= {split_date}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ split_date.\")\n",
    "\n",
    "    if val_mask.sum() == 0:\n",
    "        raise ValueError(f\"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π > {split_date}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ split_date.\")\n",
    "\n",
    "    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –ø–æ—Å–ª–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö\n",
    "    if train_mask.sum() > 0 and val_mask.sum() > 0:\n",
    "        max_train_timestamp = df.loc[train_mask, timestamp_col].max()\n",
    "        min_val_timestamp = df.loc[val_mask, timestamp_col].min()\n",
    "\n",
    "        if min_val_timestamp <= max_train_timestamp:\n",
    "            raise ValueError(\n",
    "                f\"–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ ({min_val_timestamp}) \"\n",
    "                f\"–Ω–µ –±–æ–ª—å—à–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–∏ ({max_train_timestamp}). \"\n",
    "                \"–≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø—Ä–æ–±–ª–µ–º—É —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö.\"\n",
    "            )\n",
    "\n",
    "    return train_mask, val_mask\n",
    "\n",
    "def get_split_date_from_ratio(df, ratio, timestamp_col=COL_TIMESTAMP):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –¥–∞—Ç—É —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö.\"\"\"\n",
    "    if not 0 < ratio < 1:\n",
    "        raise ValueError(f\"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –º–µ–∂–¥—É 0 –∏ 1, –ø–æ–ª—É—á–µ–Ω–æ {ratio}\")\n",
    "\n",
    "    if timestamp_col not in df.columns:\n",
    "        raise ValueError(f\"–ö–æ–ª–æ–Ω–∫–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ '{timestamp_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ DataFrame.\")\n",
    "\n",
    "    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –≤ datetime —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):\n",
    "        df = df.copy()\n",
    "        df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "\n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è\n",
    "    sorted_timestamps = df[timestamp_col].sort_values()\n",
    "    threshold_index = int(len(sorted_timestamps) * ratio)\n",
    "\n",
    "    return sorted_timestamps.iloc[threshold_index]\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –ü–ê–ô–ü–õ–ê–ô–ù–ê\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –≤ processed –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"–ü–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤\n",
    "    required_files = [TRAIN_FILENAME, TEST_FILENAME, USER_DATA_FILENAME, BOOK_DATA_FILENAME, \n",
    "                     BOOK_GENRES_FILENAME, GENRES_FILENAME, BOOK_DESCRIPTIONS_FILENAME]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not Path(f).exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(f\"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã: {missing_files}\")\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    merged_df, book_genres_df, _, descriptions_df = load_and_merge_data()\n",
    "\n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–Ω–∂–µ–Ω–µ—Ä–∏—é —Ñ–∏—á –ë–ï–ó –∞–≥—Ä–µ–≥–∞—Ç–æ–≤\n",
    "    featured_df = create_features(merged_df, book_genres_df, descriptions_df, include_aggregates=False)\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –≤—ã–≤–æ–¥–∞\n",
    "    processed_path = PROCESSED_DATA_DIR / PROCESSED_DATA_FILENAME\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ parquet –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\n",
    "    print(f\"\\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ {processed_path}...\")\n",
    "    featured_df.to_parquet(processed_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "    print(\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")\n",
    "\n",
    "    # –ü–µ—á–∞—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
    "    train_rows = len(featured_df[featured_df[COL_SOURCE] == VAL_SOURCE_TRAIN])\n",
    "    test_rows = len(featured_df[featured_df[COL_SOURCE] == VAL_SOURCE_TEST])\n",
    "    total_features = len(featured_df.columns)\n",
    "\n",
    "    print(\"\\n–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
    "    print(f\"  - –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏: {train_rows:,}\")\n",
    "    print(f\"  - –¢–µ—Å—Ç–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏: {test_rows:,}\")\n",
    "    print(f\"  - –í—Å–µ–≥–æ —Ñ–∏—á: {total_features}\")\n",
    "    print(f\"  - –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {processed_path}\")\n",
    "    \n",
    "    return featured_df\n",
    "\n",
    "def train():\n",
    "    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º.\"\"\"\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    processed_path = PROCESSED_DATA_DIR / PROCESSED_DATA_FILENAME\n",
    "\n",
    "    if not processed_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {processed_path}. \"\n",
    "            \"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ prepare_data().\"\n",
    "        )\n",
    "\n",
    "    print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ {processed_path}...\")\n",
    "    featured_df = pd.read_parquet(processed_path, engine=\"pyarrow\")\n",
    "    print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(featured_df):,} —Å—Ç—Ä–æ–∫ —Å {len(featured_df.columns)} —Ñ–∏—á–∞–º–∏\")\n",
    "\n",
    "    # –†–∞–∑–¥–µ–ª—è–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –Ω–∞–±–æ—Ä—ã\n",
    "    train_set = featured_df[featured_df[COL_SOURCE] == VAL_SOURCE_TRAIN].copy()\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫\n",
    "    if COL_TIMESTAMP not in train_set.columns:\n",
    "        raise ValueError(\n",
    "            f\"–ö–æ–ª–æ–Ω–∫–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ '{COL_TIMESTAMP}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ. \"\n",
    "            \"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫.\"\n",
    "        )\n",
    "\n",
    "    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –≤ datetime —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "    if not pd.api.types.is_datetime64_any_dtype(train_set[COL_TIMESTAMP]):\n",
    "        train_set[COL_TIMESTAMP] = pd.to_datetime(train_set[COL_TIMESTAMP])\n",
    "\n",
    "    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
    "    print(f\"\\n–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º {TEMPORAL_SPLIT_RATIO}...\")\n",
    "    split_date = get_split_date_from_ratio(train_set, TEMPORAL_SPLIT_RATIO, COL_TIMESTAMP)\n",
    "    print(f\"–î–∞—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: {split_date}\")\n",
    "\n",
    "    train_mask, val_mask = temporal_split_by_date(train_set, split_date, COL_TIMESTAMP)\n",
    "\n",
    "    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "    train_split = train_set[train_mask].copy()\n",
    "    val_split = train_set[val_mask].copy()\n",
    "\n",
    "    print(f\"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(train_split):,} —Å—Ç—Ä–æ–∫\")\n",
    "    print(f\"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(val_split):,} —Å—Ç—Ä–æ–∫\")\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å\n",
    "    max_train_timestamp = train_split[COL_TIMESTAMP].max()\n",
    "    min_val_timestamp = val_split[COL_TIMESTAMP].min()\n",
    "    print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {max_train_timestamp}\")\n",
    "    print(f\"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {min_val_timestamp}\")\n",
    "\n",
    "    if min_val_timestamp <= max_train_timestamp:\n",
    "        raise ValueError(\n",
    "            f\"–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ ({min_val_timestamp}) \"\n",
    "            f\"–Ω–µ –±–æ–ª—å—à–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–∏ ({max_train_timestamp}).\"\n",
    "        )\n",
    "    print(\"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ–π–¥–µ–Ω–∞: –≤—Å–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –ø–æ—Å–ª–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö\")\n",
    "\n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ (–¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö)\n",
    "    print(\"\\n–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏...\")\n",
    "    train_split_with_agg = add_aggregate_features(train_split.copy(), train_split)\n",
    "    val_split_with_agg = add_aggregate_features(val_split.copy(), train_split)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º train_split –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ç–æ–≤!\n",
    "\n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ–º train_split –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è)\n",
    "    print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...\")\n",
    "    train_split_final = handle_missing_values(train_split_with_agg, train_split)\n",
    "    val_split_final = handle_missing_values(val_split_with_agg, train_split)\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ (X) –∏ —Ç–∞—Ä–≥–µ—Ç (y)\n",
    "    exclude_cols = [\n",
    "        COL_SOURCE,\n",
    "        TARGET,\n",
    "        COL_PREDICTION,\n",
    "        COL_TIMESTAMP,\n",
    "    ]\n",
    "    features = [col for col in train_split_final.columns if col not in exclude_cols]\n",
    "\n",
    "    # –ò—Å–∫–ª—é—á–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è object –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Ñ–∏—á–∞–º–∏ –º–æ–¥–µ–ª–∏\n",
    "    non_feature_object_cols = train_split_final[features].select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    features = [f for f in features if f not in non_feature_object_cols]\n",
    "\n",
    "    X_train = train_split_final[features]\n",
    "    y_train = train_split_final[TARGET]\n",
    "    X_val = val_split_final[features]\n",
    "    y_val = val_split_final[TARGET]\n",
    "\n",
    "    print(f\"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ —Ñ–∏—á–∏: {len(features)}\")\n",
    "\n",
    "    # –û–±—É—á–∞–µ–º –µ–¥–∏–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "    print(\"\\n–û–±—É—á–µ–Ω–∏–µ LightGBM –º–æ–¥–µ–ª–∏...\")\n",
    "    model = lgb.LGBMRegressor(**LGB_PARAMS)\n",
    "\n",
    "    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã fit —Å –∫–æ–ª–±—ç–∫–æ–º —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n",
    "    fit_params = LGB_FIT_PARAMS.copy()\n",
    "    fit_params[\"callbacks\"] = [lgb.early_stopping(stopping_rounds=EARLY_STOPPING_ROUNDS)]\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=fit_params[\"eval_metric\"],\n",
    "        callbacks=fit_params[\"callbacks\"],\n",
    "    )\n",
    "\n",
    "    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "    val_preds = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    mae = mean_absolute_error(y_val, val_preds)\n",
    "    print(f\"\\n–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "    model_path = MODEL_DIR / MODEL_FILENAME\n",
    "    model.booster_.save_model(str(model_path))\n",
    "    print(f\"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}\")\n",
    "\n",
    "    print(\"\\n–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\")\n",
    "    \n",
    "    return model, rmse, mae\n",
    "\n",
    "def predict():\n",
    "    \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞.\"\"\"\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    processed_path = PROCESSED_DATA_DIR / PROCESSED_DATA_FILENAME\n",
    "\n",
    "    if not processed_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {processed_path}. \"\n",
    "            \"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ prepare_data().\"\n",
    "        )\n",
    "\n",
    "    print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ {processed_path}...\")\n",
    "    featured_df = pd.read_parquet(processed_path, engine=\"pyarrow\")\n",
    "    print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(featured_df):,} —Å—Ç—Ä–æ–∫ —Å {len(featured_df.columns)} —Ñ–∏—á–∞–º–∏\")\n",
    "\n",
    "    # –†–∞–∑–¥–µ–ª—è–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –Ω–∞–±–æ—Ä—ã\n",
    "    train_set = featured_df[featured_df[COL_SOURCE] == VAL_SOURCE_TRAIN].copy()\n",
    "    test_set = featured_df[featured_df[COL_SOURCE] == VAL_SOURCE_TEST].copy()\n",
    "\n",
    "    print(f\"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –Ω–∞–±–æ—Ä: {len(train_set):,} —Å—Ç—Ä–æ–∫\")\n",
    "    print(f\"–¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: {len(test_set):,} —Å—Ç—Ä–æ–∫\")\n",
    "\n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏ –Ω–∞ –í–°–ï–• —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö)\n",
    "    print(\"\\n–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á –Ω–∞ –≤—Å–µ—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    test_set_with_agg = add_aggregate_features(test_set.copy(), train_set)\n",
    "\n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ–º train_set –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è)\n",
    "    print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è...\")\n",
    "    test_set_final = handle_missing_values(test_set_with_agg, train_set)\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ (–∏—Å–∫–ª—é—á–∞–µ–º source, target, prediction, timestamp –∫–æ–ª–æ–Ω–∫–∏)\n",
    "    exclude_cols = [\n",
    "        COL_SOURCE,\n",
    "        TARGET,\n",
    "        COL_PREDICTION,\n",
    "        COL_TIMESTAMP,\n",
    "    ]\n",
    "    features = [col for col in test_set_final.columns if col not in exclude_cols]\n",
    "\n",
    "    # –ò—Å–∫–ª—é—á–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è object –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Ñ–∏—á–∞–º–∏ –º–æ–¥–µ–ª–∏\n",
    "    non_feature_object_cols = test_set_final[features].select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    features = [f for f in features if f not in non_feature_object_cols]\n",
    "\n",
    "    X_test = test_set_final[features]\n",
    "    print(f\"–§–∏—á–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {len(features)}\")\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "    model_path = MODEL_DIR / MODEL_FILENAME\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {model_path}. \" \n",
    "            \"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ train().\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\n–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}...\")\n",
    "    model = lgb.Booster(model_file=str(model_path))\n",
    "\n",
    "    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "    print(\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\")\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    # –û–±—Ä–µ–∑–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Ä–µ–π—Ç–∏–Ω–≥–∞ [0, 10]\n",
    "    clipped_preds = np.clip(test_preds, PREDICTION_MIN_VALUE, PREDICTION_MAX_VALUE)\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª submission\n",
    "    submission_df = test_set[[COL_USER_ID, COL_BOOK_ID]].copy()\n",
    "    submission_df[COL_PREDICTION] = clipped_preds\n",
    "\n",
    "    submission_path = SUBMISSION_DIR / SUBMISSION_FILENAME\n",
    "\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"\\n–§–∞–π–ª submission —Å–æ–∑–¥–∞–Ω –≤: {submission_path}\")\n",
    "    print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: min={clipped_preds.min():.4f}, max={clipped_preds.max():.4f}, mean={clipped_preds.mean():.4f}\")\n",
    "    \n",
    "    return submission_df, clipped_preds\n",
    "\n",
    "def validate():\n",
    "    \"\"\"–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ submission.\"\"\"\n",
    "    print(\"–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞ submission...\")\n",
    "\n",
    "    try:\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ñ–∞–π–ª submission\n",
    "        test_df = pd.read_csv(TEST_FILENAME)\n",
    "        sub_df = pd.read_csv(SUBMISSION_DIR / SUBMISSION_FILENAME)\n",
    "\n",
    "        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É\n",
    "        assert len(sub_df) == len(test_df), f\"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–ª–∏–Ω—ã submission. –û–∂–∏–¥–∞–ª–æ—Å—å {len(test_df)}, –ø–æ–ª—É—á–µ–Ω–æ {len(sub_df)}.\"\n",
    "        print(\"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –ø—Ä–æ–π–¥–µ–Ω–∞.\")\n",
    "\n",
    "        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö\n",
    "        assert (\n",
    "            not sub_df[COL_PREDICTION].isna().any()\n",
    "        ), f\"–ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ '{COL_PREDICTION}'.\"\n",
    "        print(\"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–æ–π–¥–µ–Ω–∞.\")\n",
    "\n",
    "        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–∞–±–æ—Ä –ø–∞—Ä (user_id, book_id) —Å–æ–≤–ø–∞–¥–∞–µ—Ç\n",
    "        test_keys = (\n",
    "            test_df[[COL_USER_ID, COL_BOOK_ID]]\n",
    "            .copy()\n",
    "            .set_index([COL_USER_ID, COL_BOOK_ID])\n",
    "        )\n",
    "        sub_keys = (\n",
    "            sub_df[[COL_USER_ID, COL_BOOK_ID]]\n",
    "            .copy()\n",
    "            .set_index([COL_USER_ID, COL_BOOK_ID])\n",
    "        )\n",
    "\n",
    "        assert test_keys.index.equals(\n",
    "            sub_keys.index\n",
    "        ), \"–ù–∞–±–æ—Ä –ø–∞—Ä (user_id, book_id) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ç–µ—Å—Ç–æ–≤—ã–º –Ω–∞–±–æ—Ä–æ–º.\"\n",
    "        print(\"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–∞—Ä (user_id, book_id) –ø—Ä–æ–π–¥–µ–Ω–∞.\")\n",
    "\n",
    "        # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "        assert (\n",
    "            sub_df[COL_PREDICTION]\n",
    "            .between(PREDICTION_MIN_VALUE, PREDICTION_MAX_VALUE)\n",
    "            .all()\n",
    "        ), f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [{PREDICTION_MIN_VALUE}, {PREDICTION_MAX_VALUE}].\"\n",
    "        print(\"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π [0, 10] –ø—Ä–æ–π–¥–µ–Ω–∞.\")\n",
    "\n",
    "        print(\"\\n–í–∞–ª–∏–¥–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞! –§–∞–π–ª submission –∏–º–µ–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.\")\n",
    "        return True\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞: {e}. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç.\")\n",
    "        return False\n",
    "    except AssertionError as e:\n",
    "        print(f\"–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}\")\n",
    "        return False\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# –ó–ê–ü–£–°–ö –ü–ê–ô–ü–õ–ê–ô–ù–ê - –í–´–ë–ï–†–ò–¢–ï –ù–£–ñ–ù–´–ï –Ø–ß–ï–ô–ö–ò –î–õ–Ø –ó–ê–ü–£–°–ö–ê\n",
    "# =============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞\n",
    "# –í—ã–ø–æ–ª–Ω–∏—Ç–µ —è—á–µ–π–∫–∏ –Ω–∏–∂–µ –ø–æ –ø–æ—Ä—è–¥–∫—É:\n",
    "\n",
    "# %%\n",
    "# 1. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•\n",
    "print(\"üéØ –®–ê–ì 1: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•\")\n",
    "featured_df = prepare_data()\n",
    "\n",
    "# %%\n",
    "# 2. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò\n",
    "print(\"üéØ –®–ê–ì 2: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò\")\n",
    "model, rmse, mae = train()\n",
    "\n",
    "# %%\n",
    "# 3. –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "print(\"üéØ –®–ê–ì 3: –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\")\n",
    "submission_df, predictions = predict()\n",
    "\n",
    "# %%\n",
    "# 4. –í–ê–õ–ò–î–ê–¶–ò–Ø\n",
    "print(\"üéØ –®–ê–ì 4: –í–ê–õ–ò–î–ê–¶–ò–Ø\")\n",
    "validation_success = validate()\n",
    "\n",
    "# %%\n",
    "# 5. –†–ï–ó–£–õ–¨–¢–ê–¢–´\n",
    "print(\"üéØ –®–ê–ì 5: –†–ï–ó–£–õ–¨–¢–ê–¢–´\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {len(featured_df):,} —Å—Ç—Ä–æ–∫\")\n",
    "print(f\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: RMSE = {rmse:.4f}, MAE = {mae:.4f}\")\n",
    "print(f\"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {len(predictions):,} –ø—Ä–æ–≥–Ω–æ–∑–æ–≤\")\n",
    "print(f\"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è: {'–ü–†–û–ô–î–ï–ù–ê' if validation_success else '–ù–ï –ü–†–û–ô–î–ï–ù–ê'}\")\n",
    "print(f\"üìÅ –§–∞–π–ª submission: {SUBMISSION_DIR / SUBMISSION_FILENAME}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üîß –ó–∞–ø—É—Å–∫ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤\n",
    "# –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏:\n",
    "\n",
    "# %%\n",
    "# –¢–û–õ–¨–ö–û –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•\n",
    "# featured_df = prepare_data()\n",
    "\n",
    "# %%\n",
    "# –¢–û–õ–¨–ö–û –û–ë–£–ß–ï–ù–ò–ï\n",
    "# model, rmse, mae = train()\n",
    "\n",
    "# %%\n",
    "# –¢–û–õ–¨–ö–û –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï\n",
    "# submission_df, predictions = predict()\n",
    "\n",
    "# %%\n",
    "# –¢–û–õ–¨–ö–û –í–ê–õ–ò–î–ê–¶–ò–Ø\n",
    "# validate()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "\n",
    "# %%\n",
    "# –ü–†–û–°–ú–û–¢–† –ü–û–î–ì–û–¢–û–í–õ–ï–ù–ù–´–• –î–ê–ù–ù–´–•\n",
    "if 'featured_df' in locals():\n",
    "    print(\"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "    print(f\"–†–∞–∑–º–µ—Ä: {featured_df.shape}\")\n",
    "    print(f\"–ö–æ–ª–æ–Ω–∫–∏: {list(featured_df.columns)}\")\n",
    "    print(\"\\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:\")\n",
    "    display(featured_df.head())\n",
    "\n",
    "# %%\n",
    "# –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô\n",
    "if 'predictions' in locals():\n",
    "    print(\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:\")\n",
    "    print(f\"Min: {predictions.min():.4f}\")\n",
    "    print(f\"Max: {predictions.max():.4f}\") \n",
    "    print(f\"Mean: {predictions.mean():.4f}\")\n",
    "    print(f\"Std: {predictions.std():.4f}\")\n",
    "    \n",
    "    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(predictions, bins=50, alpha=0.7, color='skyblue')\n",
    "    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥–æ–≤')\n",
    "    plt.xlabel('–†–µ–π—Ç–∏–Ω–≥')\n",
    "    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2189f8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞...\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞: 2894\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Ç–æ—Ä–æ–≥–æ —Ñ–∞–π–ª–∞...\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫ –∏–∑ –≤—Ç–æ—Ä–æ–≥–æ —Ñ–∞–π–ª–∞: 2894\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤...\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–π 0-2: 0\n",
      "–û–±–Ω–æ–≤–ª–µ–Ω–æ –¥—Ä—É–≥–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π: 2894\n",
      "–û–∫—Ä—É–≥–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤...\n",
      "–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤...\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ /home/evstigneva/nto252/submission2.12.25_1.csv...\n",
      "–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω. –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: 2894\n",
      "\n",
      "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–π—Ç–∏–Ω–≥–∞–º:\n",
      "–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: 2\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: 10\n",
      "–°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: 7.82\n",
      "–ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: 8.0\n",
      "\n",
      "–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:\n",
      "   user_id  book_id  rating_predict\n",
      "0      281  2461928               8\n",
      "1     1250    31957               7\n",
      "2     4241   196603               8\n",
      "3     5140   468894              10\n",
      "4     7781  2141951               7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_ratings(base_file, update_file, output_file):\n",
    "    \"\"\"\n",
    "    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–∏ –∏–∑ –¥–≤—É—Ö —Ñ–∞–π–ª–æ–≤ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –ø—Ä–∞–≤–∏–ª–∞–º.\n",
    "    \n",
    "    Args:\n",
    "        base_file: –ø—É—Ç—å –∫ –ø–µ—Ä–≤–æ–º—É CSV —Ñ–∞–π–ª—É —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
    "        update_file: –ø—É—Ç—å –∫–æ –≤—Ç–æ—Ä–æ–º—É CSV —Ñ–∞–π–ª—É —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\n",
    "        output_file: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. –û—Ç–∫—Ä—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–π CSV —Ñ–∞–π–ª\n",
    "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞...\")\n",
    "    try:\n",
    "        df_base = pd.read_csv(base_file)\n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞: {len(df_base)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 2. –û—Ç–∫—Ä—ã–≤–∞–µ–º –≤—Ç–æ—Ä–æ–π CSV —Ñ–∞–π–ª\n",
    "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Ç–æ—Ä–æ–≥–æ —Ñ–∞–π–ª–∞...\")\n",
    "    try:\n",
    "        df_update = pd.read_csv(update_file)\n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫ –∏–∑ –≤—Ç–æ—Ä–æ–≥–æ —Ñ–∞–π–ª–∞: {len(df_update)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤—Ç–æ—Ä–æ–≥–æ —Ñ–∞–π–ª–∞: {e}\")\n",
    "        return\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
    "    required_base_cols = ['user_id', 'book_id', 'rating_predict']\n",
    "    required_update_cols = ['user_id', 'book_id', 'rating_predict']\n",
    "    \n",
    "    if not all(col in df_base.columns for col in required_base_cols):\n",
    "        print(f\"–ü–µ—Ä–≤—ã–π —Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã: {required_base_cols}\")\n",
    "        return\n",
    "    \n",
    "    if not all(col in df_update.columns for col in required_update_cols):\n",
    "        print(f\"–í—Ç–æ—Ä–æ–π —Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã: {required_update_cols}\")\n",
    "        return\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "    print(\"–°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\")\n",
    "    update_dict = {}\n",
    "    for _, row in df_update.iterrows():\n",
    "        key = (row['user_id'], row['book_id'])\n",
    "        update_dict[key] = row['rating_predict']\n",
    "    \n",
    "    # 3-4. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏\n",
    "    print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤...\")\n",
    "    \n",
    "    processed_count = 0\n",
    "    updated_count = 0\n",
    "    \n",
    "    for idx, row in df_base.iterrows():\n",
    "        key = (row['user_id'], row['book_id'])\n",
    "        original_rating = row['rating_predict']\n",
    "        \n",
    "        if key in update_dict:\n",
    "            rating_predict = update_dict[key]\n",
    "            \n",
    "            # 3. –ï—Å–ª–∏ —Ä–µ–π—Ç–∏–Ω–≥ –æ—Ç 0 –¥–æ 2, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ + rating_predict\n",
    "            if 0 <= original_rating <= 2:\n",
    "                new_rating = (original_rating + rating_predict) / 2\n",
    "                df_base.at[idx, 'rating_predict'] = new_rating\n",
    "                processed_count += 1\n",
    "            \n",
    "            # 4. –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏–∑–º–µ–Ω—è–µ–º –Ω–∞ –¥–µ–ª—å—Ç—É\n",
    "            else:\n",
    "                delta = rating_predict - original_rating\n",
    "                new_rating = original_rating + delta\n",
    "                df_base.at[idx, 'rating_predict'] = new_rating\n",
    "                updated_count += 1\n",
    "    \n",
    "    print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–π 0-2: {processed_count}\")\n",
    "    print(f\"–û–±–Ω–æ–≤–ª–µ–Ω–æ –¥—Ä—É–≥–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {updated_count}\")\n",
    "    \n",
    "    # 5. –û–∫—Ä—É–≥–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Ä–µ–π—Ç–∏–Ω–≥–∞\n",
    "    print(\"–û–∫—Ä—É–≥–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤...\")\n",
    "    df_base['rating_predict'] = df_base['rating_predict'].apply(lambda x: round(x) if not pd.isna(x) else x)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–π—Ç–∏–Ω–≥–∏ –≤ –¥–æ–ø—É—Å—Ç–∏–º–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1-10)\n",
    "    print(\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤...\")\n",
    "    df_base['rating_predict'] = df_base['rating_predict'].clip(lower=0, upper=10)  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω—É–∂–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω\n",
    "    \n",
    "    df_base['rating_predict'] = df_base['rating_predict'].replace(9, 10)\n",
    "\n",
    "    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –Ω–æ–≤—ã–π CSV —Ñ–∞–π–ª\n",
    "    print(f\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ {output_file}...\")\n",
    "    try:\n",
    "        df_base.to_csv(output_file, index=False)\n",
    "        print(f\"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω. –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(df_base)}\")\n",
    "        \n",
    "        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
    "        print(\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–π—Ç–∏–Ω–≥–∞–º:\")\n",
    "        print(f\"–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: {df_base['rating_predict'].min()}\")\n",
    "        print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: {df_base['rating_predict'].max()}\")\n",
    "        print(f\"–°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {df_base['rating_predict'].mean():.2f}\")\n",
    "        print(f\"–ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥: {df_base['rating_predict'].median()}\")\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 5 —Å—Ç—Ä–æ–∫\n",
    "        print(\"\\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:\")\n",
    "        print(df_base.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}\")\n",
    "        return\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç–∏ –∫ –≤–∞—à–∏–º —Ñ–∞–π–ª–∞–º\n",
    "    base_file = \"/home/evstigneva/Zagr/sample_submission.csv\"  # –ü–µ—Ä–≤—ã–π CSV —Ñ–∞–π–ª\n",
    "    update_file = \"/home/evstigneva/Zagr/submission30.11.25_7.csv\"  # –í—Ç–æ—Ä–æ–π CSV —Ñ–∞–π–ª —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\n",
    "    output_file = \"/home/evstigneva/nto252/submission2.12.25_1.csv\"  # –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª\n",
    "    \n",
    "    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É\n",
    "    process_ratings(base_file, update_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615fe16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_torch (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
